{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec8d2edb-24ca-4fe7-90bc-7d37740d305d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 머신러닝을 통한 효율적인 동료 그룹 형성 및 가치평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128267c-6ee4-48b2-8243-b303f2423a24",
   "metadata": {},
   "source": [
    "## 한양대 경제금융대학 석사 2기 이세준"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32fd6a-669a-4385-af8e-620f242f215f",
   "metadata": {},
   "source": [
    "### - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e5a1e-804f-4437-a68d-f6c8fbaf814c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"유니콘.png\" width=700>\n",
    "출처 : 유니콘 기업가치 평가에 대한 논의와 시사점 (조성훈 2020, 자본시장연구원)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b332983-7ab0-4aa3-bc5b-05220392846e",
   "metadata": {},
   "source": [
    "유니콘 이란, 기업가치가 10억달러 이상인 비공개 스타트업을 의미한다.<br> \n",
    "허나 조성훈(2020)은 스타트업 회사들은 정확한 가치평가에 어려움을 겪으며, 상장 이후에 퍼포먼스가 매우 떨어진다는 점을 지적하였다.<br>\n",
    "이에 따라, 비단 스타트업 뿐만 아니라, 여러 회사들의 가치평가에 대한 중요성이 대두되며<br>\n",
    "비슷한 동료 그룹 형성 및 CFFA (Strategically Comparable Firm-Finding Algorithm) 을 바탕으로 매출액 등의 평가를 직접 해 보고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5f522f-cd3b-422c-af4d-4361036e4c36",
   "metadata": {
    "tags": []
   },
   "source": [
    "<ul>\n",
    "    <li>미국 기업이 공시하는 10-K 보고서의 Item 1 Business description을 통하여 기업을 분류하고자 하였다. </li>\n",
    "    <li>확보할 수 있는 10-K 보고서는 2,734개 기업의 보고서 였으나, 컴퓨터의 성능 및 시간 등으로 인하여 2,276개의 기업을 추출하였다. </li>\n",
    "    <li>분류된 기업을 바탕으로 비슷한 기업 특성(성장성, 매출성) 등을 이용하여 가치를 평가하고자 하는 목표 기업의 매출액, 기업 가치 등을 추정하고자 하였다.</li>\n",
    "    <li>허나, 선행연구와의 데이터 수 차이, 기한 등의 문제인지 완벽한 분류가 되지 않은 것으로 보인다.</li>\n",
    "    <li>P/S ratio (매출액 대비 종가) 예측에서도 좋은 성능을 보인 기업이 존재하였으나 대체적으로 그렇지 않은 것 처럼 보였다.</li>\n",
    "    <li>기업 분류 시, 혹은 기업가치 추정 시 현재의 데이터 셋에 더 맞출 수 있는 다른 방법이 필요한 것으로 사료된다.</li>\n",
    "    <li>선행 연구와의 차별점을 위해 여러 방법을 시도하였으나, 결과가 좋지 않아서, 단순 선행 연구 두개를 합치는 것이 아닌 다른 방법을 찾아 보는것이 중요할 것 같으며<br>\n",
    "    Spherical K-means clustering의 cluster 수에 대해서도 적합한 수준을 찾는 방법 고려등도 필요해 보인다.</li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17555e30-2dc3-4ef1-9ad3-b0133c46d1ef",
   "metadata": {},
   "source": [
    "### - Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e6c3c-2521-4158-881b-3200d8ad9a1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<ol>\n",
    "<b><li>SIC, NAICS에 대한 비판, 새로운 산업 분류 방법에 대한 연구들</li></b>\n",
    "    미국 정부는 1937년에 SIC 를 통하여 기업들의 산업 분류를 시작하였으며, 이후 북미(North America) 기준으로 미국, 캐나다, 멕시코 산업을 기반으로 1997년 NAICS를 개발하였다. 이후 Standard and Poor’s and Morgan Stanley Capital International는 GICS를 개발하였으며, 이는 financial analysts들이 재정적으로 비교 가능한 회사들을 기준으로 결정할 수 있도록 설계되었다. 학계에서는 Fama and French(1997) 이 Fama-French(FF) 산업 분류라고 하여 기존 SIC를 재 분류하여 5~49까지의 범위로 나누었다. <br>\n",
    "    하지만 SIC, NAICS, GICS 등등은 한계점이 존재한다. 현대 경제가 빠르게 변화함에 따라 기업은 그 어느때보다 빠르고 쉽게 산업에 진입하거나 퇴출할 수 있고, 새로운 산업역시 더 빠른속도로 등장하고 성장하게 된다. 이러한 상황에서 기존의 산업 분류 등은 manager 가 그들의 산업 영역 등을 결정하거나, 그들의 전략적인 투자 등의 방향성에 방해가 되게 된다. Information System 분야의 빠른 발전으로 인해, 연구자들은 많은 양의 빅데이터 기반 컴퓨팅을 통하여 산업을 분류하고자 한다. Fang et al.(2013)은 기존의 SIC, NAICS는 회사 간의 산업 유사성을 가정하지 않아 정확하지 않다고 비판을 하였으며, Chowdhuri et al.(2014) 는 서로 다른 XBRL(eXtensible Business Reporting Language) 을 기반으로 서로의 유사성을 이용하며, Yang et al.(2019)는 그래프 유사도를 활용하여 spectral clustering algorithim과 결합하여 분류하였고. Xu et al.(2020) 노동을 LinkedIn 프로파일의 데이터 세트를 이용하여 산업분류를 하였다.  \n",
    "<b><li>텍스트 마이닝을 통한 산업 분류</li></b>\n",
    "    Hoberg and Phillips(2010) 은 텍스트 마이닝을 기반으로 10-K 보고서에서 제품 설명이 유사한 기업을 합병하는 경우 더 성공적인 결과를 경험한다는 것을 보였으며, Hanley and Hoberg(2010) 은 IPO 가격이론을 SEC Edgar 웹사이트 전망 공시를 검토하기 위해 유사성 측정을 통하여 활용하였다. Hoberg and Phillips(2016)은 기업의 10-K 보고서의 business description을 바탕으로 도출된 텍스트 기반 네트워크 산업 분류(TNIC)를 개발하였고, 코사인 유사도를 바탕으로 기업의 busniess description이 다른 기업들과 비교되는 정도를 파악하여 300개 산업을 바탕으로 피어 기업을 rival/competitor 로 구분하였다. 허나 Kim et al.(2022)는 One-dimensional 로 비교하는 것은 저체 산업의 근접성과 관계를 추론할 수 없다고 지적하였으며, 텍스트 분석에서 발생하는 고차원성의 문제를 해결하기 위하여, Radovanovic et al.(2010)의 말과 같이 단순 고차원 벡터로부터 코사인 유사도 측정은 high dimension 문제가 발생하기 때문에, Baldi and Hornik(1989)에 의해 처음 도입된 심층 신경망 기반 차원성 감소 기법인 autoencoder 를 활용하여 새로운 산업분류를 하였고, TNIC 에 비해 성능이 개선되었음을 밝혔다.\n",
    "<b><li>비교 가능 회사를 사용한 가치 추정</li></b>\n",
    "    Comparalbe Company Analysis (CCA)는 유사한 산업이나 시장에서 경쟁하고 있는 다른 회사들과 비교하여 특정 회사 (target firm) 의 가치를 평가하는 방법이다. 분석가는 target firm 과 유사한 사업 모델, 시장 규모, 성장 전망 등을 가진 회사들을 선정하고, 주당 수익(EPS), 주가수익비율(P/E), 기업가치 대비 EBITDA 비율 등과 같은 재무지표를 사용하여, 처음에 선정한 유사 회사들의 평균 또는 중앙값을 통하여 target firm 의 재무지표를 추정한다. Bowman and Bush (2007)은 CCA를 바탕으로 규모가 유사한 경우 정확한 시장 베타 추정치를 제공한다는 것을 확인하였으며, 영업 레버리지와 배당금 지급 비율등을 제어해야 한다고 강조하였다. Stulman (2020) 은 전체 산업이 과대평가되거나 과소평가된다면 정확하지 않을 수 있으며, 회계 정책과 비교가능한 회사 자체를 기반으로 배수를 조작하는 것이 쉬울 수 있다고 언급하였다. \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7b85e-1abc-43cb-b1ac-3e9531c227b3",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    다음의 두 가지 대표 논문을 바탕으로 분석을 진행해 보고자 한다.<br>\n",
    "    <b>1. An artificial intelligence-enabled industry classification and its interpretation (Kim et al. 2022)</b> <br>\n",
    "    SEC Edgar 를 통해 2013~2016의 3,640 firm, 14,560 표본 수를 구성하였다.\n",
    "    10-K business description 에서 businesses and products 설명 시 유사한 단어를 사용할 것으로 예상하였고, 구별할 수 있는 특징을 추출하여 <br>\n",
    "    텍스트 기반 기계학습을 통하여 산업을 분류하였다. 빈도수가 높은 상위 2000개의 unique words 를 기반으로 전체 word vector를 형성하였으며 <br>\n",
    "    아래의 그림과 같이 각 firm 의 빈도 2000개 단어에 word vector 가 존재하는 경우 1, word vector 가 존재하지 않는 경우 0으로 binary vector V를 형성하였다. <br>\n",
    "    이후 생성된 binary vector 는 2000-length 이므로 high dimension 문제가 존재하기에, Autoencoder 를 통하여 dimension을(2000-500-125-10-125-500-2000) 축소 후 회복 하는 작업을 거친 후 <br>\n",
    "    Spherical k-means clustering을 통하여 코사인 유사도가 높은 기업들로 300개의 산업을 분류하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c8729-619e-4154-8897-00fc13ae66d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"binary 사진.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5d0e3f-edc6-4b27-bbd1-6e06458779a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"분류된 case.png\" width=700>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src='autoencoder효능.png' width=700>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e949f2-46db-4b47-aaa5-ce0e5d32cb88",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    위 그림과 같이 기존의 FF12 분류와, SIC 보다 더 정확하게 healthcare 관련된 회사들을 잘 분류했다는 것을 확인 할 수 있으며 <br>\n",
    "    오른쪽 사진은 autoencoder 를 사용한 경우, 코사인 유사도의 분포가 더 symmetrically 하므로 더 정확한 분류가 가능함을 암시한다.<br>\n",
    "    따라서 위 논문은, business descriptions 을 통하여 회사의 산업을 분류하는 것이 이전의 분류 방식보다 더 정확함을 밑의 사진을 통하여 암시하며, <br>\n",
    "    autoencoder를 활용한 방식이 이전의 text-based classification을 더 향상시킬 수 있음을 암시한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c46756-a003-4bfb-b3a6-a1bc845da427",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"across-within.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d26ef-5f52-43af-ab4a-d65f69d0fe18",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <b>2. Strategically Comparable Firm Finding Algorithm (CFFA): A Strategic Management Approach to Valuation (W.P, Kang 2021)</b> <br>\n",
    "    기존의 Comparable Company Analysis (CA) 의 일반화된 방식으로 전략적 자원이 비슷한 기업들로 peer group을 형성하고 <br>\n",
    "    회귀 분석을 기반으로 CCA와 같이 동료 회사의 평균을 넣는 것이 아닌, peer group의 기업 특성, 데이터 등을 전부 가중취 회귀하며 비선형 회귀, 재무 데이터의 곱 등을 전부 활용 가능하다. <br>\n",
    "    뿐만 아니라, 회귀분석 기반인 이 알고리즘은 연구자의 선호, 방식에 따라 IV (도구변수 추정법), 머신 러닝 및 빅 데이터 분석을 통합 가능하다.\n",
    "    아래의 보이는 표가 대표적인 CFFA의 방법으로 A,B가 peer group, C가 우리가 P/S ratio를 추정하고자 하는 target firm 일 때, <br>\n",
    "    이미 알려진 A,B,C 의 growth, profitability 의 데이터를 기반으로, (growth : asset growth, sales growth, earnings growth 등) <br>\n",
    "    (profitability : EV/EBITDA, return on asset (ROA), return on equity (ROE) 등) A,B와 C를 회귀하여, 가중치의 값이 1, 0.5 라는 것이 계산이 된다면, <br>\n",
    "    P/S ratio는 2*1 + 6*0.5 = 5 라는 것이 계산이 되게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bcaad3-e45e-40f2-9090-55fd5204ab5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<table>\n",
    "    <caption style='text-align:center;'> <b>[Wa, Wb] = [1, 0.5] </b></caption>\n",
    "    <tr>\n",
    "        <th> </th> <th>A</th> <th>B</th> <th>C (target)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Growth  </td> <td>1%</td> <td>2%</td> <td>2%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Profitability </td> <td>2%</td> <td>6%</td> <td>5%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>P/S ratio </td> <td>2%</td> <td>6%</td> <td>? -> 5%</td>\n",
    "    </tr>\n",
    "    \n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719d001-f505-49b1-9d3a-459d3a1aa964",
   "metadata": {},
   "source": [
    "$$ \\vec{y} = x\\vec{\\beta} + \\vec{\\epsilon}$$ <br>\n",
    "$\\vec{\\beta}$ : (portfolio) weight to construct a synthetic comparable firm<br>\n",
    "$x\\vec{\\beta}$ : constructed attributes of the synthetic firm<br>\n",
    "$\\vec{\\epsilon}$ : errors between the attributes of the target and the synthetic firm<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760739cb-82c9-48db-97b5-22a61dc363d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - Data, Method, Code, Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8fe13-cc0f-4772-bbb5-d50c2dab2183",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"data1.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435d6b6-38a7-4757-970c-0f99cbdeab83",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    이전의 두 가지 대표 논문을 기반으로 Kim et al.(2022) 방법을 토대로 전략적 자원이 비슷한 기업(peer group)을 형성하였으며 <br>\n",
    "    이 peer group 을 바탕으로 Kang(2021) 의 CFFA 알고리즈을 토대로 가치 평가를 진행하고자 한다. <br>\n",
    "    허나, 선행 연구들과의 차별점을 위하여, 여러가지 방법을 시도하였으나, 결과가 좋지 못하였다. <br>\n",
    "    - 뉴욕타임즈 api 와 SEC api 이용, 대중이 인식하는 산업과 사업보고서의 산업을 분류 시도하였다. 허나, 뉴욕타임즈 api의 호출 수가 적고, 확보한 SEC 10-K의 기업이 검색이 안되는 경우가 너무 많아서 <b>실패</b>하였다.\n",
    "    - 텍스트 마이닝 후, 단순 단어의 빈도수 뿐만 아니라 구글 자연어 처리 Bert를 이용하여 중요도 순으로 추출 시도하였다.<br>\n",
    "    허나, 15966개의 10-K 보고서에 대해 한 문서 추출도 bert 에서 오래 걸리며, common words가 bert 단어집 상 중요도가 가장 높게 책정되어 있어서 <b>실패</b>하였다.\n",
    "    - 본 분석에서는 2013~2019까지의 15,932 보고서, 2,276개 기업을 사용하였는데 이는 시간, 성능 등의 한계로 어쩔 수 없었으며, <br>\n",
    "    선행 연구 (Hogg : 1997~2008, 68,302, 5691 기업 사용 ,Kim : 2013~2016, 14,560, 3,640 기업 사용) 비해 <b>열악한</b> 퍼포먼스를 보인다. <br>\n",
    "    - 기존 선행연구와 마찬가지로 SEC 10-K 바탕으로 business description에서 자연어 추출 후 binary vector 를 autoencoder 처리 한 후에 Spherical K-means clustering으로 분류하였으나, 차별점이 없다는 매우 큰 한계점을 지닌다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d2f9c-8db5-4311-8f5f-98434d6364e0",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <b>1. 10-K</b> <br>\n",
    "    SEC-api 를 기반으로 전체 기업의 CIK(기업의 키)를 전부 내려받고, 각 CIK별로 필요한 연도의 10-K 보고서의 주소를 받았다.<br>\n",
    "    <img src=\"10-K-1.png\" width=500> <br>\n",
    "    위 사진에서 FY는 회계연도 ACCN은 10-K 보고서의 주소, CIK는 기업의 키 값, NAME은 회사이름, TIC는 기업의 티커를 의미한다.\n",
    "    이후 10-K 보고서의 주소를 바탕으로 Web Crawling Scripts 를 이용하여, 단어를 추출하고자 하였으며, <br>\n",
    "    <img src=\"10-K-2.png\" width=500> <br>\n",
    "    위 사진과 같이, Business description은 Item 1, Item 1A 에 나타나게 된다. 그렇기에 Default 형태의 기업들은 Part 1, Item 1. Business 부터 Item 1B. Unresolved Staff Comments 직전 까지를 추출하면 된다.<br>\n",
    "    하지만 Business의 복수 형 등의 변형은 or 문 등으로 찾기 편하지만, 기본 SEC Form 을 따르지 않고, 기업만의 방식을 따른 뒤 고지하는 경우가 존재한다... <br>\n",
    "    <img src=\"10-K-3.png\" width=500> <br>\n",
    "    이 경우는 어쩔 수 없이 직접 부문부문을 추정해야 하여 예외처리를 하게 된다. 그로 인하여 시간이 많이 소요되게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258e7eb-535e-419d-b058-1c93a3f4128f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<ol>\n",
    "    <b>2. nltk, collections 자연어 추출</b> <br>\n",
    "    15,932 10-K 보고서를 멀티 프로세스를 이용하여 Web crawling 하여 text를 추출하였다.<br>\n",
    "    그 중 기업의 전략적 자원을 식별하기 위하여 명사, 고유명사 등만 추출을 하였으며<br>\n",
    "    각 보고서 장 마다 table of contents 가 계속 들어가고, company, companys 가 너무 많이 출현하기에, 배제하였다.<br>\n",
    "    기존의 선행연구에서는 지리적 단어, 전체 빈도 20% 이상을 차지하는 단어는 제외하였으나, 이 분석에서는 지리적 단어는 많이 추출되지 않았고, <br>\n",
    "    전체 빈도의 20% 이상을 차지하는 경우가 존재하지 않아 선행연구를 따르지 않았다. <br>\n",
    "    선행 연구 에서는 빈도 수가 높은 2000개 단어를 추출하였으나, 본 분석에서는 1000개도 못 채우는 경우가 너무 많이 존재하여서, 1000개를 기준으로 잡았다. <br>\n",
    "    <img src=\"nltk.png\" width=500> <br>\n",
    "    위 사진은 CIK가 320193인 Apple 회사의 2013년 상위 단어 목록이며, software, application 등 전략적 자원을 잘 식별하는 것 으로 보인다. <br>\n",
    "    아래의 코드는 1. ACCN과 CIK를 다운받는 코드와, 2. 확보한 ACCN 기반으로, 각 사업보고서의 Business description에서 상위 1000개 단어를 추출하는 코드이다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468fa185-e6b1-4e09-b1c3-71be9b802f06",
   "metadata": {},
   "source": [
    "### 1. ACCN-CIK 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4184b5-18a5-44bd-894d-209919c8978f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ACCN - CIK 결합\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "url = 'https://www.sec.gov/files/company_tickers_exchange.json' #회사 티커 받는 SEC-api\n",
    "headers = {'User-Agent': 'Mozilla'}\n",
    "res = requests.get(url, headers=headers)\n",
    "cik_list = res.json()\n",
    "cik_df = pd.DataFrame(cik_list['data'], columns=cik_list['fields'])\n",
    "cik_df['cik'] = cik_df['cik'].apply(lambda x: f'{x:010d}')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#10-K form 에서 CIK와 10-K form 주소 따오기\n",
    "for i in range(0,len(cik_df) ): #len(cik_df) 예정\n",
    "    cik = cik_df['cik'][i]\n",
    "    name = cik_df['name'][i]\n",
    "    ticker =  cik_df['ticker'][i]\n",
    "    url = 'https://data.sec.gov/api/xbrl/companyfacts/CIK'+cik+'.json' #cik를 획득 하여 https://www.sec.gov/Archives/edgar/data/0000320193/0001193125-09-214859.txt 와 같이 cik-10k 형식 추출 예정\n",
    "    headers = {'User-Agent': 'Mozilla'}\n",
    "    res = requests.get(url, headers=headers)\n",
    "\n",
    "    if res.status_code == 200:\n",
    "        data = res.json()\n",
    "\n",
    "        # facts 항목 가져오기\n",
    "        facts = data.get('facts', {})\n",
    "        if len(facts) >0:\n",
    "            key1 = list(facts.keys())[0]\n",
    "            key2 = list(facts[key1].keys())[0]\n",
    "            key3 = list(facts[key1][key2]['units'].keys())[0]\n",
    "            it =facts[key1][key2]['units'][key3]\n",
    "            selected_items = []\n",
    "            for item in it:\n",
    "                if item.get('form') == '10-K':\n",
    "                    fy = item.get('fy', '')\n",
    "                    accn = item.get('accn', '')\n",
    "                    selected_items.append({'FY': fy, 'ACCN': accn, 'CIK': cik, 'NAME': name, 'TIC' : ticker})\n",
    "\n",
    "            df1 = pd.DataFrame(selected_items)\n",
    "            df = pd.concat([df, df1], ignore_index=True)\n",
    "    else:\n",
    "        print('URL 찾지못함')\n",
    "        print(cik, name)\n",
    "    \n",
    "#사용할 연도가 전부 존재하는 기업만 추출, 2013~2019의 보고서가 전부 존재해야됨\n",
    "required_year = [2013,2014,2015, 2016, 2017, 2018, 2019]\n",
    "df = df[df['FY'].isin(required_year)].groupby('CIK').filter(lambda x: x['FY'].nunique() == len(required_year)) #required_year 값이 존재하는 것만 추출 후, 전부 존재하는것만 len로 추출함\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.to_csv('ALL_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579dee25-ba0e-4b63-9a69-7b4cb879212a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\이세준\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def Bert(words):\n",
    "    # BERT 임베딩 생성\n",
    "    inputs = tokenizer(words, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    word_embeddings = outputs.last_hidden_state.mean(1)\n",
    "\n",
    "    # TF-IDF 계산\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(words)\n",
    "    tfidf_scores = np.array(tfidf_matrix.sum(axis=0)).flatten()\n",
    "\n",
    "    # 각 단어의 중요도 평가\n",
    "    word_importance = []\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            importance = tfidf_scores[vectorizer.vocabulary_[word]] * torch.norm(word_embeddings[i])\n",
    "            word_importance.append((word, importance))\n",
    "        except KeyError:\n",
    "            # 어휘 사전에 없는 단어는 무시\n",
    "            pass\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # 단어별로 중요도 합산\n",
    "    word_importance_sum = defaultdict(float)\n",
    "    for word, importance in word_importance:\n",
    "        word_importance_sum[word] += importance.item() # torch.Tensor를 float로 변환\n",
    "\n",
    "    # 중요도에 따라 정렬\n",
    "    sorted_importance = sorted(word_importance_sum.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    words, _ = zip(*sorted_importance)\n",
    "\n",
    "    # 품사 태깅\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "    # 명사와 자연어\n",
    "    filtered_words = [word for word, tag in tagged_words if tag in ('NN', 'NNP')]\n",
    "    filtered_words1 = [word for word in filtered_words if word not in ['table', 'contents', 'company', 'companys']]\n",
    "    top_1000_natural_words = filtered_words1[:1000]\n",
    "    ddf = pd.DataFrame(top_1000_natural_words, columns=['Bert'])\n",
    "    return ddf\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84182b81-b72a-49e4-895b-fe1def894e11",
   "metadata": {},
   "source": [
    "### 2. 사업보고서의 Business description에서 상위 1000개 단어를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ec3820-bac6-419e-b015-296ee17356c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import Counter\n",
    "import logging\n",
    "import sys, traceback\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "def textmaking(start_num, end_num): #Business description 부분에서 단어 추출하기 위한 함수\n",
    "    try:\n",
    "        df = pd.read_csv('ALL_df.csv', index_col=0)\n",
    "        print(start_num, '~', end_num) #멀티프로세스가 잘 되고 있는지 확인하기 위한 용도\n",
    "\n",
    "        #한번만 설치\n",
    "        #nltk.download('punkt')\n",
    "        #nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "        #word_df = pd.DataFrame()\n",
    "        word_df = pd.DataFrame(columns=['FY', 'CIK', 'NAME', 'word', 'count'])\n",
    "        #ii=0\n",
    "        #if len(word_df) != 0:\n",
    "        #    ii=word_df.loc[len(word_df)-1, 'i']+1\n",
    "\n",
    "        for i in range(start_num, end_num):#15966, len(df)):\n",
    "            intel = 0\n",
    "            morgan = 0\n",
    "            key = 0\n",
    "            ge=0\n",
    "            gen=0\n",
    "            att=0\n",
    "            city=0\n",
    "            edison = 0\n",
    "            cardinal=0\n",
    "            entergy=0\n",
    "            forrester =0\n",
    "            wy=0\n",
    "            MRTX = 0\n",
    "            clean = 0\n",
    "            freddie = 0\n",
    "            fy = df['FY'][i]\n",
    "            cik= df['CIK'][i]\n",
    "            name= df['NAME'][i].replace('/', '-')\n",
    "            name= name.replace('\\\\', '-')\n",
    "            ACCN = df['ACCN'][i]\n",
    "            ACCN1 = df['ACCN'][i].replace('-', '')\n",
    "            url = 'https://www.sec.gov/Archives/edgar/data/'+str(cik)+'/'+str(ACCN1)+'/'+str(ACCN)+'.txt'\n",
    "\n",
    "            headers = {'User-Agent': 'Mozilla'}\n",
    "            res = requests.get(url, headers=headers)\n",
    "            text=[]\n",
    "            if res.status_code == 200:\n",
    "                # BeautifulSoup을 사용하여 HTML 파싱\n",
    "                if i>=256 and i<= 258:\n",
    "                    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "                    intel = 1\n",
    "                elif (i>=319 and i<=321) or (i>=2010 and i<=2016): #overview 이후 financial 까지 해야되는 경우\n",
    "                    soup = BeautifulSoup(res.text, 'lxml')\n",
    "                    morgan = 1\n",
    "                elif (i>=344 and i<=349) or (i>=1922 and i<=1925): #about 뭐시기로 끊고 바로 legal 까지 대문자 형태로 구분이 되는 경우\n",
    "                    if i<349:\n",
    "                        first = res.text.find(\"ABOUT GENERAL ELECTRIC\")\n",
    "                    elif i<1925:\n",
    "                        first = res.text.find(\"ABOUT MOLINA HEALTHCARE\")\n",
    "                    end = res.text.find(\"LEGAL PROCEEDINGS\")\n",
    "                    tt = res.text[first:end]\n",
    "                    soup = BeautifulSoup(tt, 'lxml')\n",
    "                    ge=1\n",
    "                elif (i>=595 and i<= 601):\n",
    "                    soup = BeautifulSoup(res.text, 'lxml')\n",
    "                    city = 1\n",
    "                    if i == 595:\n",
    "                        city = 2\n",
    "                elif (i>=1710 and i<=1715): #EDISON 형식, CORPORATE STRUCTURE, INDUSTRY AND OTHER INFORMATION 부터 staff 까지\n",
    "                    soup = BeautifulSoup(res.text, 'lxml')\n",
    "                    edison = 1\n",
    "                elif (i>=1753 and i<=1757) or (i>=2031 and i<=2033):#business general 꼴\n",
    "                    soup = BeautifulSoup(res.text, 'lxml')\n",
    "                    cardinal=1\n",
    "                elif (i>=1877 and i<=1883): #OUrbusiness 형식으로 되어있어서 구분이 힘든 애들\n",
    "                    soup = BeautifulSoup(res.text, 'lxml')\n",
    "                    wy = 1\n",
    "                elif (i>=2003 and i<=2009): #entergy 형식들, 아예 형식이 너무 다름\n",
    "                    try: #lxml이 안먹는 애들\n",
    "                        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "                    except:\n",
    "                        soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "                    entergy = 1\n",
    "                elif (i>=5214 and i<=5220):\n",
    "                    try: #lxml이 안먹는 애들\n",
    "                        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "                    except:\n",
    "                        soup = BeautifulSoup(res.text, 'lxml')\n",
    "                    MRTX = 1\n",
    "                elif i == 181:\n",
    "                    first = res.text.find(\"BUSINESS SUMMARY\")\n",
    "                    end = res.text.find(\"LEGAL PROCEEDINGS\")\n",
    "                    tt = res.text[first:end]\n",
    "                    soup = BeautifulSoup(tt, 'lxml')\n",
    "                    ge=1\n",
    "                elif i== 7309:\n",
    "                    first = res.text.find(\"THE COMPANY\")\n",
    "                    end = res.text.find(\"LEGAL PROCEEDINGS\")\n",
    "                    tt = res.text[first:end]\n",
    "                    soup = BeautifulSoup(tt, 'lxml')\n",
    "                    ge=1\n",
    "                elif (i>=9933 and i<= 9937): #part i general 로 시작하는 형태\n",
    "                    soup = BeautifulSoup(res.text, 'lxml')\n",
    "                    forrester = 1\n",
    "                elif (i>=10600 and i <= 10604):\n",
    "                    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "                    freddie = 1\n",
    "                elif (i>=14100 and i<=14101):\n",
    "                    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "                    clean = 1\n",
    "                elif (i>=14450 and i<=14451):\n",
    "                    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "                    gen = 1\n",
    "                else:\n",
    "                    try: #lxml이 안먹는 애들\n",
    "                        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "                    except:\n",
    "                        soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "                # 모든 스크립트와 스타일 태그 제거\n",
    "                for script in soup([\"script\", \"style\"]):\n",
    "                    script.extract()\n",
    "\n",
    "                # 텍스트 추출\n",
    "                text = soup.get_text().strip()\n",
    "                # 텍스트에서 구두점 및 공백 제거 \n",
    "                text = re.sub(r'[^\\w\\s]','', text)\n",
    "                text = re.sub(r'\\xa0', ' ', text)    \n",
    "                text = re.sub(r'\\n', ' ', text)\n",
    "                text = re.sub(r'\\s+', ' ', text)\n",
    "                text = text.lower()\n",
    "\n",
    "                if intel == 1:\n",
    "                    start_index = text.find(\"introduction to our business\")\n",
    "                    first_met = text.find(\"availability of company information\") #처음으로 만나는 끝 직전\n",
    "                    if ((start_index - first_met) < 0) and ((start_index - first_met) > -1000): #목차에서 서로 나타난다는 뜻임 unresolved 항목은 어떻게든 반드시 목차에선 저 문구대로 나올 예정\n",
    "                        #슬라이스 후 item1 business 찾고 unresovlved 그 이후 찾기\n",
    "                        text = text[start_index+len('item'):]\n",
    "                        start_index = text.find(\"introduction to our business\")\n",
    "                        text = text[start_index+len('item'):]\n",
    "                        end_index = text.find(\"availability of company information\") #처음으로 만나는 끝 직전\n",
    "                        text=text[:end_index]\n",
    "                       # print('완료 : ', url, len(text))\n",
    "                        key = 1\n",
    "                    elif ((start_index - first_met) > 0) or ((start_index - first_met) < -1000): #목차에서 item1 business 형태가 아니라는 뜻임 아마 바로 항목으로 갈꺼임 / -1000은 목차가 없는 경우 방지 위해서 즉 바로 item1 나오는거\n",
    "                        #슬라이스 후 바로 unresolved 찾기\n",
    "                        text = text[start_index+len('item'):]\n",
    "                        start_index = text.find(\"introduction to our business\")\n",
    "                        text = text[start_index+len('item'):]\n",
    "                        end_index = text.find(\"availability of company information\") #처음으로 만나는 끝 직전\n",
    "                        text=text[:end_index]\n",
    "                        #print('완료 : ', url, len(text))\n",
    "                        key = 1\n",
    "                elif i == 2023:\n",
    "                    start_index = text.find(\"business overview\")\n",
    "                    first_met = text.find(\"selected financial data\") #처음으로 만나는 끝 직전\n",
    "                    if ((start_index - first_met) < 0) and ((start_index - first_met) > -1000): #목차에서 서로 나타난다는 뜻임 unresolved 항목은 어떻게든 반드시 목차에선 저 문구대로 나올 예정\n",
    "                        #슬라이스 후 item1 business 찾고 unresovlved 그 이후 찾기\n",
    "                        text = text[start_index+len('item'):]\n",
    "                        start_index = text.find(\"business overview\")\n",
    "                        text = text[start_index+len('item'):]\n",
    "                        end_index = text.find(\"selected financial data\") #처음으로 만나는 끝 직전\n",
    "                        text=text[:end_index]\n",
    "                        #print('완료 : ', url, len(text))\n",
    "                        key = 1\n",
    "                    elif ((start_index - first_met) > 0) or ((start_index - first_met) < -1000): #목차에서 item1 business 형태가 아니라는 뜻임 아마 바로 항목으로 갈꺼임 / -1000은 목차가 없는 경우 방지 위해서 즉 바로 item1 나오는거\n",
    "                        #슬라이스 후 바로 unresolved 찾기\n",
    "                        text = text[start_index+len('item'):]\n",
    "                        start_index = text.find(\"overview\")\n",
    "                        text = text[start_index+len('item'):]\n",
    "                        end_index = text.find(\"selected financial datas\") #처음으로 만나는 끝 직전\n",
    "                        text=text[:end_index]\n",
    "                        #print('완료 : ', url, len(text))\n",
    "                        key = 1\n",
    "                elif (i >= 15680 and i<= 15685): #갑자기 얘 혼자만 overview 부터 시작함\n",
    "                    start_index = text.find(\"overview\")\n",
    "                    text = text[start_index+len('item'):]\n",
    "                    end_index = text.find(\"unresolved staff comments\")\n",
    "                    text=text[:end_index]\n",
    "                    #print('완료 : ', url, len(text))\n",
    "                    key = 1\n",
    "                elif gen == 1:\n",
    "                    start_index = text.find('company background')\n",
    "                    text = text[start_index:]\n",
    "                    end_index = text.find('unresolved staff comments')\n",
    "                    text = text[:end_index]\n",
    "                    key = 1\n",
    "                elif morgan == 1:\n",
    "                    start_index = text.find(\"overview\")\n",
    "                    first_met = text.find(\"selected financial data\") #처음으로 만나는 끝 직전\n",
    "                    if ((start_index - first_met) < 0) and ((start_index - first_met) > -1000): #목차에서 서로 나타난다는 뜻임 unresolved 항목은 어떻게든 반드시 목차에선 저 문구대로 나올 예정\n",
    "                        #슬라이스 후 item1 business 찾고 unresovlved 그 이후 찾기\n",
    "                        text = text[start_index+len('item'):]\n",
    "                        start_index = text.find(\"overview\")\n",
    "                        text = text[start_index+len('item'):]\n",
    "                        end_index = text.find(\"selected financial data\") #처음으로 만나는 끝 직전\n",
    "                        text=text[:end_index]\n",
    "                        #print('완료 : ', url, len(text))\n",
    "                        key = 1\n",
    "                    elif ((start_index - first_met) > 0) or ((start_index - first_met) < -1000): #목차에서 item1 business 형태가 아니라는 뜻임 아마 바로 항목으로 갈꺼임 / -1000은 목차가 없는 경우 방지 위해서 즉 바로 item1 나오는거\n",
    "                        #슬라이스 후 바로 unresolved 찾기\n",
    "                        text = text[start_index+len('item'):]\n",
    "                        start_index = text.find(\"overview\")\n",
    "                        text = text[start_index+len('item'):]\n",
    "                        end_index = text.find(\"selected financial datas\") #처음으로 만나는 끝 직전\n",
    "                        text=text[:end_index]\n",
    "                        #print('완료 : ', url, len(text))\n",
    "                        key = 1\n",
    "                elif MRTX == 1:\n",
    "                    pattern = r\"business\\s?overview\"\n",
    "                    match = re.search(pattern, text)\n",
    "                    if match:\n",
    "                        start_index=match.end()\n",
    "                        end_index = text[30000:].find(\"unresolved staff comments\") #처음으로 만나는 끝 직전\n",
    "                        if end_index == -1:\n",
    "                            end_index = text[30000:].find(\"unresolved sec staff comments\")\n",
    "                            if end_index== -1:\n",
    "                                end_index = text[30000:].find(\"legal proceedings\") \n",
    "                        text=text[:end_index+30000]\n",
    "                        #print('완료 : ', url, len(text))\n",
    "                        key = 1\n",
    "                elif edison == 1:\n",
    "                    start_index = text.find('corporate structure')\n",
    "                    text=text[start_index:]\n",
    "                    end_index = text[30000:].find(\"unresolved staff comments\") #처음으로 만나는 끝 직전\n",
    "                    if end_index == -1:\n",
    "                        end_index = text[30000:].find(\"unresolved sec staff comments\")\n",
    "                        if end_index== -1:\n",
    "                            end_index = text[30000:].find(\"legal proceedings\") \n",
    "                    text=text[:end_index+30000]\n",
    "                    #print('완료 : ', url, len(text))\n",
    "                    key = 1\n",
    "                elif cardinal == 1:\n",
    "                    pattern = r\"business\\s?general\"\n",
    "                    match = re.search(pattern, text)\n",
    "                    if match:\n",
    "                        start_index = match.end()\n",
    "                        first_met = text.find(\"legal proceedings\") #처음으로 만나는 끝 직전\n",
    "                        if first_met == -1:\n",
    "                            first_met = text.find(\"legal proceedings\")\n",
    "                        if ((start_index - first_met) < 0) and ((start_index - first_met) > -1000): #목차에서 서로 나타난다는 뜻임 unresolved 항목은 어떻게든 반드시 목차에선 저 문구대로 나올 예정\n",
    "                            #슬라이스 후 item1 business 찾고 unresovlved 그 이후 찾기\n",
    "                            text = text[start_index+len('item'):]\n",
    "                            match = re.search(pattern, text)\n",
    "                            if match:\n",
    "                                start_index = match.end()\n",
    "                                text = text[start_index+len('item'):]\n",
    "                                end_index = text[30000:].find(\"legal proceedings\") #처음으로 만나는 끝 직전\n",
    "                                text=text[:end_index+30000]\n",
    "                                #print('완료 : ', url, len(text))\n",
    "                                key = 1\n",
    "                        elif ((start_index - first_met) > 0) or ((start_index - first_met) < -1000): #목차에서 item1 business 형태가 아니라는 뜻임 아마 바로 항목으로 갈꺼임 / -1000은 목차가 없는 경우 방지 위해서 즉 바로 item1 나오는거\n",
    "                            #슬라이스 후 바로 unresolved 찾기\n",
    "                            text = text[start_index+len('item'):]\n",
    "                            end_index = text[30000:].find(\"legal proceedings\") #처음으로 만나는 끝 직전\n",
    "                            text=text[:end_index+30000]\n",
    "                            #print('완료 : ', url, len(text))\n",
    "                            key = 1\n",
    "                elif ge == 1:\n",
    "                    key = 1\n",
    "                    #print('완료 : ', url, len(text))\n",
    "                elif forrester == 1:\n",
    "                    pattern = r\"part\\s?i\\s?general\"\n",
    "                    match = re.search(pattern, text)\n",
    "                    if match:\n",
    "                        start_index=match.end()\n",
    "                        text = text[start_index+len('item'):]\n",
    "                        end_index = text[30000:].find(\"unresolved staff comments\") #처음으로 만나는 끝 직전\n",
    "                        if end_index == -1:\n",
    "                            end_index = text[30000:].find(\"unresolved sec staff comments\")\n",
    "                            if end_index== -1:\n",
    "                                end_index = text[30000:].find(\"legal proceedings\") \n",
    "                        text=text[:end_index+30000]\n",
    "                        #print('완료 : ', url, len(text))\n",
    "                        key = 1\n",
    "                elif wy == 1:\n",
    "                    start_index = text.find('our businesswe are')\n",
    "                    text=text[start_index:]\n",
    "                    end_index = text[30000:].find(\"unresolved staff comments\") #처음으로 만나는 끝 직전\n",
    "                    if end_index == -1:\n",
    "                        end_index = text[30000:].find(\"unresolved sec staff comments\")\n",
    "                        if end_index== -1:\n",
    "                            end_index = text[30000:].find(\"legal proceedings\") \n",
    "                    text=text[:end_index+30000]\n",
    "                    #print('완료 : ', url, len(text))\n",
    "                    key = 1\n",
    "                elif city > 0:\n",
    "                    start_index = text.find('citigroups history')\n",
    "                    text=text[start_index:]\n",
    "                    if city == 2:\n",
    "                        end_index = text.find('risk managementoverview')\n",
    "                    elif city == 1:\n",
    "                        end_index = text.find('managing global risk table of contents')\n",
    "                    text=text[:end_index]\n",
    "                    key = 1\n",
    "                    #print('완료 : ', url, len(text)) \n",
    "                elif freddie == 1:\n",
    "                    start_index = text.find('freddie mac is a gse')\n",
    "                    text=text[start_index:]\n",
    "                    end_index = text.find('we are involved as a party')\n",
    "                    text=text[:end_index]\n",
    "                    key = 1\n",
    "                    #print('완료 : ', url, len(text)) \n",
    "                elif entergy == 1:\n",
    "                    text = re.sub(r'font|style', ' ', text)\n",
    "                    start_index = text.find('entergy is an integrated energy company')\n",
    "                    text = text[start_index:]\n",
    "                    end_index = text.find('entergy arkansas inc and subsidiaries')\n",
    "                    text = text[:end_index]\n",
    "                    #print('완료 : ', url, len(text))\n",
    "                    key = 1\n",
    "                elif clean == 1:\n",
    "                    start_index = text.find('general business overview')\n",
    "                    text = text[start_index:]\n",
    "                    end_index = text.find('unresolved staff comments')\n",
    "                    text = text[:end_index]\n",
    "                    #print('완료 : ', url, len(text))\n",
    "                    key = 1\n",
    "                else:\n",
    "                    pattern = r\".*?i{0,2}tems?\\s?(1|i|(1 i))\\s?(1a\\s)?(2|1a|and\\s2\\s?)?(discussion of|description of|our)?\\s?(the\\s?)?busines\"\n",
    "                    match = re.search(pattern, text)\n",
    "                    if match:\n",
    "                        start_index = match.end()\n",
    "                        first_met = text.find(\"unresolved staff comments\") #처음으로 만나는 끝 직전\n",
    "                        if first_met == -1:\n",
    "                            first_met = text.find(\"unresolved sec staff comments\")\n",
    "                            if first_met== -1:\n",
    "                                first_met = text.find(\"item 2 properties\") \n",
    "                                if first_met == -1:\n",
    "                                    first_met = text.find(\"legal proceedings\") \n",
    "                        if ((start_index - first_met) < 0) and ((start_index - first_met) > -1000): #목차에서 서로 나타난다는 뜻임 unresolved 항목은 어떻게든 반드시 목차에선 저 문구대로 나올 예정\n",
    "                            #슬라이스 후 item1 business 찾고 unresovlved 그 이후 찾기\n",
    "                            text = text[start_index+len('item'):]\n",
    "                            match = re.search(pattern, text)\n",
    "                            if match:\n",
    "                                start_index = match.end()\n",
    "                                text = text[start_index+len('item'):]\n",
    "                                end_index = text[10000:].find(\"unresolved staff comments\") #처음으로 만나는 끝 직전\n",
    "                                if end_index == -1:\n",
    "                                    end_index = text[10000:].find(\"unresolved sec staff comments\")\n",
    "                                    if end_index== -1:\n",
    "                                        end_index = text[10000:].find(\"item 2 properties\") \n",
    "                                        if end_index == -1:\n",
    "                                            end_index = text[10000:].find(\"legal proceedings\") \n",
    "                                text=text[:end_index+10000]\n",
    "                                #print('완료 : ', url, len(text))\n",
    "                                key = 1\n",
    "                        elif ((start_index - first_met) > 0) or ((start_index - first_met) < -1000): #목차에서 item1 business 형태가 아니라는 뜻임 아마 바로 항목으로 갈꺼임 / -1000은 목차가 없는 경우 방지 위해서 즉 바로 item1 나오는거\n",
    "                            #슬라이스 후 바로 unresolved 찾기\n",
    "                            text = text[start_index+len('item'):]\n",
    "                            end_index = text[10000:].find(\"unresolved staff comments\") #처음으로 만나는 끝 직전\n",
    "                            if end_index == -1:\n",
    "                                end_index = text[10000:].find(\"unresolved sec staff comments\")\n",
    "                                if end_index== -1:\n",
    "                                    end_index = text[10000:].find(\"item 2 properties\") \n",
    "                                    if end_index == -1:\n",
    "                                        end_index = text[10000:].find(\"legal proceedings\") \n",
    "                            text=text[:end_index+10000]\n",
    "                            #print('완료 : ', url, len(text))\n",
    "                            key = 1\n",
    "                    else:\n",
    "                        print('error2 : ', url)\n",
    "\n",
    "                #설치는 이미 했으면 필요는 없음\n",
    "                #nltk.download('punkt')\n",
    "                #nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "                if(key == 1):\n",
    "                    # 텍스트에서 구두점 및 공백 제거 (이미 수행한 부분)\n",
    "                    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "                    # 단어를 소문자로 변환하고 공백을 기준으로 분할 (이미 수행한 부분)\n",
    "                    words = text.lower().split()\n",
    "                    \n",
    "                    #혹시 모르니 정제한 txt 파일을 저장을 해 놓자\n",
    "                    text_str = ''.join(text)\n",
    "                    file = open(\"C:/Users/이세준/자산평가/text/\"+str(name)+\"-\"+str(fy)+\".txt\", \"w\", encoding=\"utf-8\")\n",
    "                    file.write(text_str)\n",
    "                    file.close()\n",
    "                    \n",
    "                    #ddf = Bert(words)\n",
    "                    \n",
    "                    #배제하는방법 ex table, contents 등\n",
    "                    words = [word for word in words if word not in ['table', 'contents', 'company', 'companys']]\n",
    "\n",
    "                    # 토큰화된 단어에 품사 태깅 수행\n",
    "                    tagged_words = pos_tag(words)\n",
    "\n",
    "                    # 명사 및 고유 명사 추출\n",
    "                    nouns = [word for word, pos in tagged_words if pos in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "\n",
    "                    word_counts = Counter(nouns)\n",
    "\n",
    "                    # 가장 빈도가 높은 상위 20개 단어 추출\n",
    "                    top_20_words = word_counts.most_common(1000)      #논문 따라 2000으로 증량 필요할것, 또한 한다면 risk factors 까지가 아닌, item 2까지일수도\n",
    "                    selected_items = []\n",
    "                    for word, count in top_20_words:\n",
    "                        selected_items.append({'FY': fy, 'CIK': cik, 'NAME': name, 'word' : word, 'count' : count, 'i' : i})\n",
    "\n",
    "                    df2 = pd.DataFrame(selected_items)\n",
    "                    word_df = pd.concat([word_df, df2], ignore_index=True)\n",
    "                    word_df = pd.concat([word_df, ddf], ignore_index=True)\n",
    "                    word_df.to_csv('word_df-'+str(start_num)+'.csv')\n",
    "                else:\n",
    "                    print('error3', url)\n",
    "\n",
    "            else: \n",
    "                print('통신에러')\n",
    "        word_df.to_csv('word_df-'+str(start_num)+'.csv')\n",
    "        return(word_df)\n",
    "    except:\n",
    "        logging.error(traceback.format_exc())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db3b9c-3490-48f7-9267-c5214e3deae5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using threads: 6\n",
      "4000 ~ 5000\n",
      "0 ~ 1000\n",
      "2000 ~ 3000\n",
      "1000 ~ 2000\n",
      "3000 ~ 4000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, TimeoutError\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "#ranges = [(5441, 6000), (8741, 9000), (9577, 10000), (10621, 11000), (12531, 13000), (13132, 14000), (15001, 15965)]\n",
    "ranges = [(i, i + 1000) for i in range(0, 15000, 1000)] + [(15000, 15966)]\n",
    "\n",
    "result_data = pd.DataFrame()\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    executor.map(lambda x: textmaking(*x), ranges)\n",
    "    #results = executor.map(lambda x: textmaking(*x), ranges)\n",
    "    #for group_data_s in results:\n",
    "        #result_data = pd.concat([result_data, group_data_s])\n",
    "        \n",
    "#result_data.to_csv('resutl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0264b56-22e3-4e8e-9ca2-ef11f0f11069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#혹시 멀티프로세스 오류 등으로 따로 저장되어서 병합이 필요한 경우\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def loadmerge_csv(path):\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    file_data = []\n",
    "    for file in files:\n",
    "        match = re.search(r'-(\\d+)\\.csv$', file)\n",
    "        if match:\n",
    "            number = int(match.group(1))\n",
    "            file_data.append((number, file))\n",
    "\n",
    "    #정렬\n",
    "    file_data.sort()\n",
    "\n",
    "    # 병합\n",
    "    merged_df = pd.DataFrame()\n",
    "    for _, file in file_data:\n",
    "        df = pd.read_csv(os.path.join(path, file))\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "path = os.path.dirname(os.path.realpath(__file__))\n",
    "merged_df = loadmerge_csv(path)\n",
    "merged_df = merged_df.drop(merged_df.columns[0], axis=1)\n",
    "merged_df.to_csv('word_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259b9510-0c3c-4e13-99e8-3962a60df387",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <b>3. Firm_word_vector & All_word_vector </b> <br>\n",
    "    각 기업별로 Business description의 총 길이가 다르기도 하고, 같은 기업 내에서도 연도별로 형식 길이 등이 다르다. <br>\n",
    "    그렇기에 단순 빈도수로 count를 하는 것이 아니라, 각 상위 단어에 대해서 가중치를 부여하는 Rank 방식이나 <br>\n",
    "    혹은 전체 길이에서 어느정도 빈도로 나타나는지 Ratio 등도 고려 할 만 하다. <br>\n",
    "    각 기업의 연도별 1000개 단어에 대해서 상위 출현 단어부터 가중치를 주어 모든 연도를 합친 1000개의 단어를 Firm_word_vector <b>(V)</b> 라 하고, <br>\n",
    "    전체 기업의 각 연도별 1000개 단어에 대해서 상위 출현 단어부터 가중치를 주어 모든 연도를 합친 1000개의 단어를 All_word_vector <b>(W)</b> 라 한다. <br>\n",
    "    이후 각 기업별로 W 벡터에 들어있는 단어가 존재하는 경우 1로 매칭을 해 준다.\n",
    "    <img src=\"WV.png\" width=500> <br>\n",
    "    위의 사진은 ABBOTT LABORAATORIES 의 예시이다. 만일 W vector의 첫 번째 단어인 business가 Firm_word_vector 안에 존재하는 경우 1과 0으로 임베딩 되는 V vector의 첫 번째 값은 1로 <br>\n",
    "    이를 W vector의 단어 전체에 동일하게 적용하여 binary vector V 를 획득 가능하다. <br>\n",
    "    아래의 코드는 구성한 전체 회사의 연도별 단어 집 word_df.csv를 기반으로 weight에 rank값을 준 후, 이를 기반으로 전체 단어집인 W와 word_counts 라는 각 기업별 단어집을 통해서 <br>\n",
    "    Binary vector V 를 형성하는 과정이다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70446e7e-5bc5-4751-8ca3-45977bfabf0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#20-length word vector 생성 W vector\n",
    "word_df = pd.read_csv('word_df.csv',index_col=0,low_memory=False)\n",
    "word_df['weight'] = word_df.groupby(['NAME', 'FY'])['count'].rank(ascending=True)\n",
    "wword_counts = word_df.groupby('word')['weight'].sum()\n",
    "top_20_wwords = wword_counts.sort_values(ascending=False).head(1000) #2000으로 바뀔 예정, 전체집단에 대해서 순위권 높은거 많이 든 애들임\n",
    "W = pd.DataFrame({'word' : top_20_wwords.index})\n",
    "\n",
    "W.to_csv('W.csv', index=True)\n",
    "\n",
    "#각 cik별 firm vetor 생성 \n",
    "word_counts = word_df.groupby(['CIK', 'word'])['weight'].sum().reset_index()\n",
    "\n",
    "# 각 'CIK' 별로 상위 20개 단어 선택\n",
    "top_words = word_counts.groupby('CIK').apply(lambda x: x.nlargest(1000, 'weight'))\n",
    "result_df = top_words.reset_index(drop=True)\n",
    "#result_df가 회사들 전부 해서 포함되어 있음\n",
    "\n",
    "firms = result_df['CIK'].unique() \n",
    "#result_df[result_df['CIK'] == firms[0]] 방식으로 특정 회사 firm vector 추출 가능\n",
    "\n",
    "V = pd.DataFrame()\n",
    "for i in range(0, len(firms)):\n",
    "    firm_vec = result_df[result_df['CIK'] == firms[i]]\n",
    "    W1 = W.copy()\n",
    "    W1['CIK'] = firms[i]\n",
    "    for j in range(0, len(W)):\n",
    "        if W['word'][j] in firm_vec['word'].values:\n",
    "            W1.loc[j,'vec']= 1\n",
    "        else:\n",
    "            W1.loc[j,'vec']= 0\n",
    "    V = pd.concat([V, W1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c47de36-10b8-4552-a82a-9eeb41b30485",
   "metadata": {
    "tags": []
   },
   "source": [
    "<ol>\n",
    "    <b>4. Autoencoder </b> <br>\n",
    "    - Radovanovic et al.(2010) 이 지적하였듯, Cosine similarity 를 측정 시, 고차원 벡터는 쌍 별 Cosine similarity 값이 일정하고, 분산이 0 으로 수렴 가능하다. <br>\n",
    "    - 따라서, Autoencoder 로 dimensionality reduction 사용하여 high dimension 문제를 해결하고자 한다. <br>\n",
    "    - Autoencoder 는 입력 데이터를 압축 (encoding) 후 다시 복원 (decoding) 하는 비지도 학습으로 데이터의 특징을 학습하고 , 차원을 축소하며 노이즈 제거등에 사용하는 신경망 구조 이다. <br>\n",
    "    - 학습을 시킨 후, 전체 데이터에 대해서 적용을 해야 되기에, 전체 CIK(기업들) 에 대해서 0.4의 비율로 split을 한 다음 test set에서 50%는 값 검정에 사용하였다. (train : 0.4, validation : 0.3, test : 0.3) <br>\n",
    "    - 이 때, 전체 데이터에 적용을 해야 하는데, 정확도를 위하여, 여러번 학습을 하여 가장 좋은 train set 의 학습 결과로 적용을 시키고자 한다.\n",
    "    - 20 번의 random_state 바탕으로 , Test loss 가 가장 적은 state 의 train 값으로 전체 데이터를 autoencoding 해 주었다. <br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"autoencoder1.png\" width=500>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src='autoencoder2.png' width=400>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f6704e-86f2-4f03-ba09-56722990af3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Autoencoder 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "789ebb68-61bb-4c25-87b7-ea1ed7c7c2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#autoencoder compile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "latent_dim = 1000\n",
    "\n",
    "\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(250, activation='relu'),\n",
    "            layers.Dense(50, activation='relu'),\n",
    "            layers.Dense(10, activation='relu')\n",
    "        ])\n",
    "    # 디코더 부분: 10 -> 50 -> 250 -> 1000\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "        layers.Dense(50, activation='relu'),\n",
    "        layers.Dense(250, activation='relu'),\n",
    "        layers.Dense(1000, activation='sigmoid')\n",
    "    ])\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = Autoencoder(latent_dim)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a1e08-8093-4b73-80c4-cf09d56550f8",
   "metadata": {},
   "source": [
    "### V vector 에 대하여, train을 진행한 후, 최적의 모델로 autoencoding된 V 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60eaa172-d847-4f4a-b95c-2fbeabf83f53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1422\n",
      "Test loss with random state 0: 0.14220520853996277\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1424\n",
      "Test loss with random state 1: 0.14243869483470917\n",
      "15/15 [==============================] - 0s 929us/step - loss: 0.1423\n",
      "Test loss with random state 2: 0.1423157900571823\n",
      "15/15 [==============================] - 0s 929us/step - loss: 0.1430\n",
      "Test loss with random state 3: 0.14304107427597046\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1426\n",
      "Test loss with random state 4: 0.14255671203136444\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1425\n",
      "Test loss with random state 5: 0.14250662922859192\n",
      "15/15 [==============================] - 0s 929us/step - loss: 0.1436\n",
      "Test loss with random state 6: 0.14355026185512543\n",
      "15/15 [==============================] - 0s 929us/step - loss: 0.1419\n",
      "Test loss with random state 7: 0.1418546587228775\n",
      "15/15 [==============================] - 0s 929us/step - loss: 0.1393\n",
      "Test loss with random state 8: 0.13928990066051483\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1404\n",
      "Test loss with random state 9: 0.14041471481323242\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1404\n",
      "Test loss with random state 10: 0.14041513204574585\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1382\n",
      "Test loss with random state 11: 0.13824117183685303\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1419\n",
      "Test loss with random state 12: 0.14192074537277222\n",
      "15/15 [==============================] - 0s 929us/step - loss: 0.1386\n",
      "Test loss with random state 13: 0.1385815143585205\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1413\n",
      "Test loss with random state 14: 0.14126519858837128\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1393\n",
      "Test loss with random state 15: 0.1392892301082611\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1374\n",
      "Test loss with random state 16: 0.13744954764842987\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1392\n",
      "Test loss with random state 17: 0.13920550048351288\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1386\n",
      "Test loss with random state 18: 0.1385594606399536\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1388\n",
      "Test loss with random state 19: 0.13884837925434113\n",
      "72/72 [==============================] - 0s 916us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "V = pd.read_csv('V.csv', index_col=0)\n",
    "\n",
    "def prepare_data(ciks):\n",
    "    data = []\n",
    "    for cik in ciks:\n",
    "        t = V[V['CIK'] == cik]['vec'].tolist()\n",
    "        data.append(t)\n",
    "    return np.array(data)\n",
    "\n",
    "# 여러 random_state 값으로 데이터 분할 및 평가\n",
    "best_test_loss = float('inf')\n",
    "best_random_state = -1\n",
    "\n",
    "for random_state in range(20):\n",
    "    # 데이터 분할\n",
    "    unique_ciks = V['CIK'].unique()\n",
    "    train_ciks, test_ciks = train_test_split(unique_ciks, test_size=0.4, random_state=random_state)\n",
    "    test_ciks, val_ciks = train_test_split(test_ciks, test_size=0.5, random_state=random_state)\n",
    "\n",
    "    # 데이터 준비\n",
    "    train_data = prepare_data(train_ciks)\n",
    "    val_data = prepare_data(val_ciks)\n",
    "    test_data = prepare_data(test_ciks)\n",
    "\n",
    "    # 모델 학습\n",
    "    autoencoder.fit(train_data, train_data,\n",
    "                    epochs=50,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val_data, val_data), verbose=0)\n",
    "\n",
    "    # 모델 평가\n",
    "    test_loss = autoencoder.evaluate(test_data, test_data)\n",
    "    print(f\"Test loss with random state {random_state}: {test_loss}\")\n",
    "\n",
    "    # 최적의 모델 선택\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        best_random_state = random_state\n",
    "\n",
    "# 최적의 random_state를 사용하여 전체 데이터셋에 대한 'decode' 값 산출\n",
    "train_ciks, _ = train_test_split(unique_ciks, test_size=0.4, random_state=best_random_state)\n",
    "train_data = prepare_data(train_ciks)\n",
    "autoencoder.fit(train_data, train_data, epochs=50, batch_size=256, verbose=0)\n",
    "\n",
    "all_data = prepare_data(unique_ciks)\n",
    "decoded_data = autoencoder.predict(all_data)\n",
    "\n",
    "# 결과 저장\n",
    "decode1 = [item for sublist in decoded_data for item in sublist]\n",
    "V['decode'] = pd.DataFrame(decode1)\n",
    "V.to_csv('V_best.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786465c8-0e25-4cae-b614-42a34e2f7d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' 0.3 랜덤하고 고르고 학습만 한 애들, 이후 전체 셋에대해서 fitting함\n",
    "import numpy as np\n",
    "unique_ciks = V['CIK'].unique()\n",
    "\n",
    "num_ciks_to_select = int(len(unique_ciks) * 0.3)\n",
    "\n",
    "# 0.3의 test기반 랜덤하게 선택된 cik 들\n",
    "selected_ciks = np.random.choice(unique_ciks, num_ciks_to_select, replace=False)\n",
    "\n",
    "\n",
    "train_data = []\n",
    "for a in selected_ciks:\n",
    "    t = V[V['CIK'] == a]['vec'].tolist()\n",
    "    train_data.append(t)\n",
    "autoencoder.fit(train_data, train_data, epochs=50, batch_size=256)\n",
    "\n",
    "decode_data = []\n",
    "for k in unique_ciks:\n",
    "    t = V[V['CIK'] == k]['vec'].tolist()\n",
    "    decode_data.append(t)\n",
    "decode = autoencoder.predict(decode_data)\n",
    "\n",
    "decode1 = [item for sublist in decode for item in sublist]\n",
    "V['decode'] = pd.DataFrame(decode1)\n",
    "V.to_csv('V.csv', index=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af1207-72d5-4c19-b64c-f384809b607f",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <b>5. Spherical K-means clustering </b> <br>\n",
    "    - Strehl et al.(2000) 이 언급하였듯, text data에 대해선 Euclidean distance 보다 Cosine similarity를 이용하는 것이 더 효과적이다. <br>\n",
    "    - 하지만, 이 cluster 집단 수에 대해서는 뚜렷한 기준을 찾기 어려웠다. 선행연구는 300개의 집단을 형성하였으나, 그만한 표본 수를 확보하지 못하였고, <br>\n",
    "    뚜렷하게 비교를 해 보는 것 이 목표였기에 확보한 표본기업의 SIC가 총 68개 여서 cluster 군집 수는 68개로 결정하였다. <br>\n",
    "    $$ \\text{cosine similarity} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}} $$ <br>\n",
    "    - cosine similarity는 다음과 같은 수식으로 계산되며, cosine distance = 1 - cosine similarity 로 계산된다. <br>\n",
    "    - 즉 cosine distance가 가까우면 가까울수록, 유사도는 높다는 뜻이 된다. 이를 기반으로 clustering 하는 것이 Spherical k-means clustering 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "964c2c1a-d99f-466c-87c3-48f102e4bda4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from soyclustering import SphericalKMeans\n",
    "from scipy.sparse import csr_matrix\n",
    "from soyclustering import proportion_keywords\n",
    "\n",
    "\n",
    "V = pd.read_csv('V_best.csv', index_col = 0)\n",
    "\n",
    "grouped = V.groupby('CIK')['decode'].apply(list)\n",
    "\n",
    "X = pd.DataFrame(grouped.tolist(), index=grouped.index)\n",
    "X_sparse = csr_matrix(X.values)\n",
    "\n",
    "kmeans = SphericalKMeans(n_clusters=68)\n",
    "clusters = kmeans.fit_predict(X_sparse)\n",
    "#center 바탕으로 weight 높은거 찾아볼거임\n",
    "centroids = kmeans.cluster_centers_\n",
    "#V에 그 cluster 집단까지 포함을 해줌\n",
    "V['cluster']= pd.DataFrame(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd785a24-6497-4f6c-86c8-88b0ba1bb532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_words_with_same_weight(centroid, num_top_features, words):\n",
    "    # 상위 특징의 인덱스와 그 값들을 가져옴\n",
    "    top_indices = np.argsort(-centroid)[:num_top_features]\n",
    "    top_values = centroid[top_indices]\n",
    "\n",
    "    # 추가적으로 동일한 가중치를 가진 특징들을 포함\n",
    "    last_value = top_values[-1]\n",
    "    extra_indices = np.where(centroid == last_value)[0]\n",
    "    top_indices = np.union1d(top_indices, extra_indices)\n",
    "\n",
    "    # 상위 특징에 해당하는 단어들 반환\n",
    "    return [(words[index], centroid[index]) for index in top_indices]\n",
    "\n",
    "# 상위 단어 추출\n",
    "num_top_features = 15 #상위 단어 몇개?\n",
    "top_words_per_cluster = [get_top_words_with_same_weight(centroid, num_top_features, V['word'].tolist()) for centroid in centroids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29af694b-936a-4b52-8fc3-c68b35e71b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 원하는 클러스터의 단어와 가중치 출력\n",
    "df = pd.read_csv('ALL_df.csv', index_col=0)\n",
    "\n",
    "re = pd.DataFrame(V['CIK'].unique(), columns=['CIK'])\n",
    "re['cluster'] = pd.DataFrame(clusters)\n",
    "df_cik_name = df[['CIK', 'NAME']].drop_duplicates()\n",
    "re = re.merge(df_cik_name, on='CIK', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ace6355-d472-4934-94ee-818c3de5c416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' \n",
    "점수가 필요하면 쓰자\n",
    "cluster_index = 299\n",
    "print(f\"Cluster {cluster_index}:\")\n",
    "for word, score in top_words_per_cluster[cluster_index]:\n",
    "    print(f\"  {word}: {score}\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc7934da-034e-4925-ba4c-3e97aa183dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 47 Names: AMERICAN EXPRESS CO, JPMORGAN CHASE & CO, FIFTH THIRD BANCORP, FIRST HORIZON CORP, Ally Financial Inc., BANK OF AMERICA CORP /DE/, NORTHERN TRUST CORP, KEYCORP /NEW/, TRUIST FINANCIAL CORP, STATE STREET CORP, UMB FINANCIAL CORP, PNC FINANCIAL SERVICES GROUP, INC., RAYMOND JAMES FINANCIAL INC, CITIGROUP INC, GOLDMAN SACHS GROUP INC, MORGAN STANLEY, CAPITAL ONE FINANCIAL CORP, Discover Financial Services\n",
      "Cluster 47 Top Words: business, results, ability, information, costs, time, regulations, laws, requirements, management, factors, effect, risks, condition, services, number, result, markets, conditions, capital, companies, industry, future, employees, products, cash, systems, control, operating, cost, loss, value, stock, service, terms, part, assets, state, compliance, impact, act, failure, use, performance, rates, technology, competition, securities, amount, credit, quality, reports, interest, data, statements, liability, rate, levels, government, investment, insurance, access, december, period, standards, protection, debt, events, system, income, obligations, actions, level, report, policies, losses, businesses, transactions, extent, regulation, event, provisions, equity, investments, payment, earnings, rules, increase, agencies, reporting, funds, date, subsidiaries, things, accounting, types, executive, portfolio, dividends, policy, subsidiary, asset, institutions, loan, reserve, rating, mortgage, fund\n"
     ]
    }
   ],
   "source": [
    "cluster_index = 47\n",
    "#클러스터 속하는 애들\n",
    "names_in_cluster = re[re['cluster'] == cluster_index]['NAME']\n",
    "print(f\"Cluster {cluster_index} Names:\", ', '.join(names_in_cluster))\n",
    "\n",
    "#클러스터 속하는 단어들\n",
    "top_words = [word for word, _ in top_words_per_cluster[cluster_index]]\n",
    "print(f\"Cluster {cluster_index} Top Words:\", ', '.join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45f7edb0-46e2-4321-b445-debe1af2a76d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 65 Names: CHEMED CORP, HUMANA INC, CVS HEALTH Corp, TENET HEALTHCARE CORP, Graham Holdings Co, UNIVERSAL HEALTH SERVICES INC, Adtalem Global Education Inc., UNITEDHEALTH GROUP INC, Encompass Health Corp, DLH Holdings Corp., RadNet, Inc., HCA Healthcare, Inc., U S PHYSICAL THERAPY INC /NV, Pediatrix Medical Group, Inc., AMEDISYS INC, DAVITA INC., Strategic Education, Inc., Option Care Health, Inc., PERDOCEO EDUCATION Corp, NATIONAL HEALTHCARE CORP, CENTENE CORP, Apollo Medical Holdings, Inc., COMMUNITY HEALTH SYSTEMS INC, ENSIGN GROUP, INC, Elevance Health, Inc., MOLINA HEALTHCARE, INC., AMERICAN PUBLIC EDUCATION INC, ModivCare Inc, UNIVERSAL TECHNICAL INSTITUTE INC, LINCOLN EDUCATIONAL SERVICES CORP, SELECT MEDICAL HOLDINGS CORP, Grand Canyon Education, Inc., Addus HomeCare Corp, Acadia Healthcare Company, Inc., Performant Financial Corp\n",
      "Cluster 65 Top Words: business, operations, market, ability, addition, information, costs, time, regulations, changes, laws, requirements, management, factors, effect, risks, condition, services, number, risk, result, markets, conditions, capital, future, employees, cash, sales, systems, cost, years, loss, value, service, states, terms, part, assets, state, impact, resources, use, performance, technology, competition, amount, competitors, basis, year, form, data, security, rate, levels, investment, access, period, expenses, change, variety, share\n"
     ]
    }
   ],
   "source": [
    "cluster_index = 65\n",
    "#클러스터 속하는 애들\n",
    "names_in_cluster = re[re['cluster'] == cluster_index]['NAME']\n",
    "print(f\"Cluster {cluster_index} Names:\", ', '.join(names_in_cluster))\n",
    "\n",
    "#클러스터 속하는 단어들\n",
    "top_words = [word for word, _ in top_words_per_cluster[cluster_index]]\n",
    "print(f\"Cluster {cluster_index} Top Words:\", ', '.join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "300f4072-9449-414b-bc76-5f19cf72194a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ALL_df.csv', index_col=0)\n",
    "\n",
    "re = pd.DataFrame(V['CIK'].unique(), columns=['CIK'])\n",
    "re['cluster'] = pd.DataFrame(clusters)\n",
    "df_cik_name = df[['CIK', 'NAME']].drop_duplicates()\n",
    "re = re.merge(df_cik_name, on='CIK', how='left')\n",
    "re.to_csv('cluster_name1.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e84e8828-9a2c-4685-b850-508ff5545dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5490481185262543"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('cluster_name_sic.csv', index_col=0)\n",
    "\n",
    "cluster_match_rates = {}\n",
    "\n",
    "# 각 cluster에 대해 반복\n",
    "for cluster in data['cluster'].unique():\n",
    "    # 해당 cluster에 속하는 데이터\n",
    "    cluster_data = data[data['cluster'] == cluster]\n",
    "\n",
    "    # 가장 많이 나타나는 sic_group 찾기\n",
    "    most_common_sic_group = cluster_data['sic_group'].mode()[0]\n",
    "\n",
    "    # 해당 cluster의 CIK들 중 해당 sic_group에 속하는 비율 계산\n",
    "    match_rate = len(cluster_data[cluster_data['sic_group'] == most_common_sic_group]) / len(cluster_data)\n",
    "    cluster_match_rates[cluster] = match_rate\n",
    "\n",
    "# 모든 cluster에 대한 일치율의 평균 계산\n",
    "average_match_rate = np.mean(list(cluster_match_rates.values()))\n",
    "average_match_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc1061-04e3-4da1-8260-1ea5ef5ff853",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    각 cluster 별로 SIC group와 어느정도 유사도를 보이는 지 검정을 해 본 결과 일치율은 약 54%정도로 나타난다.<br>\n",
    "    <img src=\"cluster_result.png\" width=700> <br>\n",
    "    위 사진은 cluster 47 집단에 대해서 정리한 표이며, 대부분 Financial 회사들이 이에 속하는 것으로 나타났다.<br>\n",
    "    전체 표본에서 대표적인 Financial SIC code인 6020인 회사 수가 221기업인데, cluster 62 집단에 속한 122개 기업 중 113개가 SIC가 6020인 것으로 나타났다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6cb060af-3df8-4c73-b832-a533b2c163f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' 보류, wihtin, across 가 잘 안맞음\n",
    "df1 = pd.read_csv('groupasset.csv')\n",
    "df = df1[['fyear', 'gvkey', 'at', 'OI/Sale','OI/asset','NI/Sale','NI/asset' ,'cluster', 'sic_group']].copy()\n",
    "df['at'] = pd.to_numeric(df['at'], errors='coerce')\n",
    "df['OI/Sale'] = pd.to_numeric(df['OI/Sale'], errors='coerce')\n",
    "df['OI/asset'] = pd.to_numeric(df['OI/asset'], errors='coerce')\n",
    "df['NI/Sale'] = pd.to_numeric(df['NI/Sale'], errors='coerce')\n",
    "df['NI/asset'] = pd.to_numeric(df['NI/asset'], errors='coerce')\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# 각 gvkey 별로 OI/Sale의 가중 평균 계산\n",
    "def weighted_mean(group, column, weight):\n",
    "    d = group[column]\n",
    "    w = group[weight]\n",
    "    return (d * w).sum() / w.sum()\n",
    "\n",
    "def weighted_average_std(group, column, weight):\n",
    "    std_d = group[column].std()\n",
    "    avg_w = group[weight].mean()\n",
    "    return std_d * avg_w\n",
    "# 계산할 컬럼 목록\n",
    "columns_to_calculate = ['OI/Sale', 'OI/asset', 'NI/Sale', 'NI/asset']\n",
    "\n",
    "# 각 컬럼에 대한 계산 수행\n",
    "results = {}\n",
    "\n",
    "for column in columns_to_calculate:\n",
    "    # 각 gvkey 별로 컬럼의 가중 평균 계산\n",
    "    weighted_means_by_gvkey = df.groupby(['fyear', 'gvkey']).apply(weighted_mean, column, 'at')\n",
    "\n",
    "    # 산업 전반에 걸친 가중 평균의 표준 편차 계산\n",
    "    across_industry_std = weighted_means_by_gvkey.groupby('fyear').std()\n",
    "\n",
    "    # 각 cluster 별로 가중 평균된 표준 편차 계산\n",
    "    weighted_std_by_cluster = df.groupby(['fyear', 'cluster']).apply(weighted_average_std, column, 'at')\n",
    "    industry_weighted_std = weighted_std_by_cluster.groupby('fyear').mean()\n",
    "    # 결과 저장\n",
    "    results[column] = pd.DataFrame({\n",
    "        column + '_across': across_industry_std,\n",
    "        column + '_within': industry_weighted_std\n",
    "    })\n",
    "\n",
    "# 모든 결과를 하나의 DataFrame으로 결합\n",
    "final_result = pd.concat(results.values(), axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa5af6-efaa-48b1-ae2e-66cad3c069e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<ol>\n",
    "    <b>6. CFFA </b> <br>\n",
    "    - CIK를 확보하였기에, 이를 기반으로 Compustat과 결합을 시도하였다. 그 결과 전체 2276기업에서 1702개의 기업으로 표본이 감소하였다. Compustat을 통해 받은 기업 재무 데이터는 data.csv로 저장이 되어 있다. <br>\n",
    "    - P/S ratio를 추정하기 위해 Compustat 으로 부터 감가상각비, 부동산 매각 ROE, 소득세 등의 데이터를 추출하였다. <br>\n",
    "    - P/S ratio는 분기별 종가/1주당 매출액으로 계산이 되며, 후에 EV/S ratio (시가총액 + 총 부채 + 현금 및 현금등가물 /1주당 매출액) 역시 고려 가능할 것 이다. <br>\n",
    "    - 같은 cluster로 peer group이 형성 되었으나, 정확한 추정을 위하여 그 안에서 서로 평균자산이 20% 이내로 차이나는 기업들만 고려하였고, 총 1359개 기업, 415개의 group이 형성이 되게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67302d45-4208-4254-ab01-e8ac30235f95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "회사 수 :  1702\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv') #분기보고서 기준, 분기별 데이터, 예상치로 데이터 분석하고자함\n",
    "\n",
    "# datacqtr : 몇년 몇분기\n",
    "# cik : 각 회사 cik 값\n",
    "# cluster : 이전에 분류한 cluster(산업? peer그룹?) 집단\n",
    "# sic_group : sic 3자리 기준으로 나눈 산업집단\n",
    "# actq : 현재 자산, peer 내에서 자산 비슷한 애들끼리로 분석할 예정\n",
    "# ceqq : 보통주수\n",
    "# cshoq : 발행 보통주수 -> 자회사 보통주수 제외\n",
    "# chq : Cash & Cash equivalent\n",
    "# ltq : 총 부채\n",
    "# niq : Net income\n",
    "# oibdpq : 감가상각비\n",
    "# saleq : 매출액-> 발행수인 ceqq로 나누어줘야됨\n",
    "# tstknq : 자사주 보유수\n",
    "# txtq : 소득세\n",
    "# ciy : 총포괄손익(net inocme, 매출액 등하고 겹칠듯 하여 안 쓸 예정)\n",
    "# sppey : Sale of property, 자산매출액?\n",
    "# mkvaltq : 시가총액\n",
    "# prccq : 종가\n",
    "\n",
    "df1 = df[['datacqtr', 'conm', 'cik', 'cluster', 'sic_group', 'actq' ,'prccq', 'saleq', 'cshoq' ,'mkvaltq', 'ltq', 'chq', 'sppey', 'oibdpq', 'niq', 'txtq']].copy()\n",
    "df1 = df1.dropna(subset=['datacqtr', 'conm', 'cik', 'cluster', 'sic_group', 'actq' , 'prccq', 'saleq', 'cshoq', 'mkvaltq', 'ltq', 'chq', 'niq'])\n",
    "print('회사 수 : ', len(df1['conm'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe2f112-f103-4762-a5c6-42a03c4dfa25",
   "metadata": {},
   "source": [
    "### cluster 내에서 평균 자산이 유사한 group으로 형성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db081a1-8db5-44d0-9416-7c2a596fe239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered = df1.groupby('cluster')\n",
    "\n",
    "# 각 클러스터 내에서 서브그룹 생성\n",
    "subgroups = {}\n",
    "for cluster, group in clustered:\n",
    "    # conm 별로 평균 actq 계산\n",
    "    avg_actq = group.groupby('conm')['actq'].mean()\n",
    "\n",
    "    # 서브그룹 생성\n",
    "    for name, actq in avg_actq.items():\n",
    "        lower_bound = actq * 0.8\n",
    "        upper_bound = actq * 1.2\n",
    "\n",
    "        # 유사한 actq 값을 가진 다른 conm 찾기\n",
    "        similar_companies = avg_actq[(avg_actq >= lower_bound) & (avg_actq <= upper_bound) & (avg_actq.index != name)]\n",
    "        \n",
    "        # 서브그룹에 추가 (서브그룹 내 다양성 확인)\n",
    "        if len(similar_companies) > 0:\n",
    "            subgroups.setdefault(cluster, []).append((name, similar_companies.index.tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f18aa0d-7b77-4e56-ba53-6d374e1df71e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assigned_conms = set()  # 이미 할당된 conm을 추적하기 위한 집합, 집합으로 처리하여 비교할 예정, 할당하는 경우 \n",
    "cffa_group_id = 1  # CFFA_group ID 시작\n",
    "\n",
    "for cluster, group in subgroups.items():\n",
    "    for sub in group:\n",
    "        conms_in_group = [sub[0]] + sub[1]\n",
    "\n",
    "        # 아직 할당되지 않은 conm만 처리\n",
    "        unassigned_conms = [conm for conm in conms_in_group if conm not in assigned_conms] #서브그룹 중 할당 안된 경우 이 집합 안에 들어 있을 예정\n",
    "        if len(unassigned_conms) > 1:  # 서브그룹에 최소 두 개의 다른 conm이 있어야 함\n",
    "            # CFFA_group ID 할당\n",
    "            for conm in unassigned_conms:\n",
    "                df1.loc[(df1['cluster'] == cluster) & (df1['conm'] == conm), 'CFFA_group'] = cffa_group_id\n",
    "                assigned_conms.add(conm)  # 할당된 conm 추적\n",
    "\n",
    "            cffa_group_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "363ff680-35f0-4b8f-8ac0-973cb4255034",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "할당 안된 회사 수 :  357\n"
     ]
    }
   ],
   "source": [
    "print('할당 안된 회사 수 : ', len(df1[pd.isna(df1['CFFA_group'])]['conm'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffa4abd3-deb6-4c48-a6a2-e188667979d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "할당된 회사 수 :  1345\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.dropna(subset = ['CFFA_group']).copy()\n",
    "print('할당된 회사 수 : ', len(df2['conm'].unique()))\n",
    "df2 = df2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38f4e507-6749-4393-b116-35d779d965cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#EV 생성\n",
    "df2['EV'] = df2['mkvaltq'] + df2['ltq'] + df2['chq']\n",
    "# 주당 매출액\n",
    "df2['SOE'] = df2['saleq']/df2['cshoq']\n",
    "df2['P/S'] = df2['prccq']/df2['SOE']\n",
    "df2['EV/S'] = df2['EV']/df2['SOE']\n",
    "df2['ROE'] = df2['niq']/df2['cshoq']\n",
    "df2['ROA'] = df2['niq']/df2['actq']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0e07420-9225-4db5-8610-1ac2cef7cb54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df2['diffactq'] = df2.groupby('conm')['actq'].diff()\n",
    "df2['diffsaleq'] = df2.groupby('conm')['saleq'].diff()\n",
    "df2['diffniq'] = df2.groupby('conm')['niq'].diff()\n",
    "df2 = df2.dropna(subset = ['diffactq', 'diffsaleq', 'diffniq', 'ROA', 'ROE', 'EV'])\n",
    "df2 = df2.replace([np.inf, -np.inf], np.nan)\n",
    "df2 = df2.dropna(subset=['P/S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3ec9d-21e9-468f-8329-a2365c8f03e4",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    - 각 그룹 내에서 target 회사를 반복문을 통하여 돌아가면서 선정하여 CFFA를 적용한다. 이후 계산된 MAE를 통해 정확도를 보고자 한다. <br>\n",
    "    허나 weight는 이전에 언급한 것과 같이 다변수 값이 들어가기에 정확한 수치를 추정하기 쉽지 않다. 그렇기에 코사인 유사도를 바탕으로 진행을 하였으며, <br>\n",
    "    가중치를 해 주기 위해서 peer가 두개 이상인 회사들로만 한정을 하여 분석을 진행하였다. <br>\n",
    "    $$\n",
    "X_t = \\left[ X_a, X_b, X_c, \\ldots \\right] \\cdot \\left[ W_a, W_b, W_c, \\ldots \\right]^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "P/S_t = \\left[P/S_a, P/S_b, P/S_c, \\ldots \\right] \\cdot \\left[ W_a, W_b, W_c, \\ldots \\right]^T\n",
    "$$\n",
    "    위의 첫 번째 식과 같이 ROA, ROE, EV, diffactq(자산 변화율), diffsaleq(매출액 변화율), dffniq(순 수익 변화율) 등을 이용하여 $$W_a, W_b, W_c$$ 를 추정하고 <br>\n",
    "    이를 P/S에 대입하여 target 회사의 P/S를 추정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "692dfaef-3b35-449b-b7b5-7c9d02ed00dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def caculate_weight_return(target_features, peer_group_features, peer_group_ps):\n",
    "    similarities = cosine_similarity([target_features], peer_group_features)[0]\n",
    "    # 가중치 계산 (유사도를 가중치로 사용)\n",
    "    weights = similarities / np.sum(similarities)\n",
    "    weighted_ps = np.dot(peer_group_ps, weights)\n",
    "    return weighted_ps\n",
    "\n",
    "i=0\n",
    "result = pd.DataFrame(columns=['cff_group', 'datecqtr', 'mae', 'num_peer'])\n",
    "for group in df2['CFFA_group'].unique():\n",
    "    group_data = df2[df2['CFFA_group'] == group]\n",
    "\n",
    "    #date 별로 반복문\n",
    "    for datacqtr in group_data['datacqtr'].unique():\n",
    "        datacqtr_data = group_data[group_data['datacqtr'] == datacqtr]\n",
    "\n",
    "        actual_ps_values = []\n",
    "        predicted_ps_values = []\n",
    "\n",
    "        # 각 분기에서 각 회사별로 반복\n",
    "        if len(datacqtr_data['conm'].unique()) >3:\n",
    "            for conm in datacqtr_data['conm'].unique():\n",
    "                target_company = datacqtr_data[datacqtr_data['conm'] == conm]\n",
    "                peer_group = datacqtr_data[datacqtr_data['conm'] != conm]\n",
    "\n",
    "                # 유사도 계산 및 P/S 예측을 위한 필요한 데이터 추출\n",
    "                target_features = target_company[['ROA', 'ROE', 'EV', 'diffactq', 'diffsaleq', 'diffniq']].iloc[0]\n",
    "                peer_group_features = peer_group[['ROA', 'ROE', 'EV', 'diffactq', 'diffsaleq', 'diffniq']]\n",
    "                peer_group_ps = peer_group['P/S']\n",
    "\n",
    "                # 유사도 계산 및 P/S 예측\n",
    "                predicted_ps = caculate_weight_return(target_features, peer_group_features, peer_group_ps)\n",
    "\n",
    "                # 예측된 P/S와 실제 P/S 비교\n",
    "                actual_ps = target_company['P/S'] \n",
    "\n",
    "                actual_ps_values.append(target_company['P/S'])\n",
    "                predicted_ps_values.append(predicted_ps)\n",
    "\n",
    "            date_mae = mean_absolute_error(actual_ps_values, predicted_ps_values)\n",
    "            result.loc[i,'cff_group'] = group\n",
    "            result.loc[i,'datecqtr'] = datacqtr\n",
    "            result.loc[i,'mae'] = date_mae\n",
    "            result.loc[i,'num_peer'] = len(peer_group)\n",
    "            i+=1\n",
    "result = result.sort_values(by='mae', axis=0)\n",
    "result.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ce54744-c81d-41ae-a174-d41db8ef8a09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cff_group</th>\n",
       "      <th>datecqtr</th>\n",
       "      <th>mae</th>\n",
       "      <th>num_peer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>152.0</td>\n",
       "      <td>2012Q3</td>\n",
       "      <td>0.189340</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>152.0</td>\n",
       "      <td>2012Q2</td>\n",
       "      <td>0.203527</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>405.0</td>\n",
       "      <td>2012Q3</td>\n",
       "      <td>0.225520</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>152.0</td>\n",
       "      <td>2018Q1</td>\n",
       "      <td>0.267106</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>405.0</td>\n",
       "      <td>2012Q2</td>\n",
       "      <td>0.295796</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cff_group datecqtr       mae  num_peer\n",
       "952       152.0   2012Q3  0.189340         3\n",
       "951       152.0   2012Q2  0.203527         3\n",
       "1293      405.0   2012Q3  0.225520         4\n",
       "973       152.0   2018Q1  0.267106         3\n",
       "1292      405.0   2012Q2  0.295796         4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "result = pd.read_csv('result.csv', index_col=0)\n",
    "result.head() #각 분기 별 Mae가 낮은 순서대로 정렬한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cddd63c-bddb-47d1-b0d2-ce956c3d5f77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cff_group\n",
      "234.0        1.092813\n",
      "152.0        1.185571\n",
      "155.0        1.188335\n",
      "349.0        1.190547\n",
      "186.0        1.496244\n",
      "             ...     \n",
      "36.0      2293.091490\n",
      "47.0      2363.789938\n",
      "390.0     6368.113739\n",
      "245.0     9601.634005\n",
      "248.0    12856.672689\n",
      "Name: mae, Length: 123, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "grouped_result = result.groupby('cff_group')['mae'].mean().sort_values(ascending=True)\n",
    "print(grouped_result) #각 cff_group의 mae값을 평균내서, mae가 가장 작은 집단을 추정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ed13e2d-ee8f-4803-9ea1-e338d1af6003",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mae가 낮은 3개의 군집:\n",
      "234.0: ['INTERFACE INC' 'ASTEC INDUSTRIES INC' 'WABASH NATIONAL CORP'\n",
      " 'ENCORE WIRE CORP' 'COMPASS DIVERSIFIED HOLDINGS']\n",
      "152.0: ['CABOT CORP' 'SPX TECHNOLOGIES INC' 'SONOCO PRODUCTS CO' 'ENERSYS']\n",
      "155.0: ['FULLER (H. B.) CO' 'ENVIRI CORP' 'LOUISIANA-PACIFIC CORP' 'STEPAN CO'\n",
      " 'KAISER ALUMINUM CORP' 'DARLING INGREDIENTS INC']\n",
      "\n",
      " Mae가 높은 3개의 군집:\n",
      "390.0: ['PROPHASE LABS INC' 'DELCATH SYSTEMS INC' 'CYTODYN INC' 'ORGENESIS INC']\n",
      "245.0: ['PROTHENA CORPORATION PLC' 'INTRA-CELLULAR THERAPIES INC' 'INSMED INC'\n",
      " 'ACADIA PHARMACEUTICALS INC' 'MIRATI THERAPEUTICS INC'\n",
      " 'AMICUS THERAPEUTICS INC']\n",
      "248.0: ['GYRE THERAPEUTICS INC' 'ALTIMMUNE INC' 'MOLECULAR TEMPLATES'\n",
      " 'VAXART INC' 'LUMOS PHARMA INC' 'ALAUNOS THERAPEUTICS INC'\n",
      " 'CATALYST PHARMACEUTICALS INC' 'ASSEMBLY BIOSCIENCES INC' 'VERASTEM INC']\n"
     ]
    }
   ],
   "source": [
    "top_5_cff_groups = grouped_result.head(3).index\n",
    "bottom_5_cff_groups = grouped_result.tail(3).index\n",
    "\n",
    "top_5_comn = df2[df2['CFFA_group'].isin(top_5_cff_groups)]['conm'].unique()\n",
    "bottom_5_comn = df2[df2['CFFA_group'].isin(bottom_5_cff_groups)]['conm'].unique()\n",
    "\n",
    "print(\"Mae가 낮은 3개의 군집:\")\n",
    "for cff_group in top_5_cff_groups:\n",
    "    print(f\"{cff_group}: {df2[df2['CFFA_group'] == cff_group]['conm'].unique()}\")\n",
    "\n",
    "print(\"\\n Mae가 높은 3개의 군집:\")\n",
    "for cff_group in bottom_5_cff_groups:\n",
    "    print(f\"{cff_group}: {df2[df2['CFFA_group'] == cff_group]['conm'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19210135-3ed9-4a0d-a84a-3d3a64aab228",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <b>머신러닝 모델 사용</b> <br>\n",
    "    코사인 유사도를 통한 P/S 추정 뿐만이 아닌, CFFA의 기본 알고리즘을 바탕으로, 추가 변수로 peer group의 P/S를 추가하여 머신러닝을 통해 예측을 해 보고자 한다. <br>\n",
    "    시계열 데이터 이기에, 2022년 1분기 이전과 이후로 train/test 를 나누어서 2022년 1분기,2분기,3분기,4분기에 대하여 예측을 진행하며, 이전과 동일하게 가중치들의 합 등으로 표현이 되기 위하여 peer가 두개 이상인 경우로 통일하였다. <br>\n",
    "    $$ P/S_{i,t} = Peer P/S_{i,t} + Control_{i,t} + P/S_{i,t-1} + P/S_{i,t-2} $$\n",
    "    위 식과 같은 형태로 peer group의 P/S를 넣어주며 Control은 이전 코사인 유사도에서 사용하였던 기업 변수인 ROA, ROE, EV, diffactq(자산 변화율), diffsaleq(매출액 변화율), dffniq(순 수익 변화율) 이고 <br>\n",
    "    추가적으로 직전 1,2분기의 P/S ratio도 넣어주었다. 이전과 마찬가지로 MAE를 계산하여 MAE가 훌륭한 group을 알아보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "604ff2f6-7b9b-4321-97a2-4d3393135e36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "res = pd.DataFrame()\n",
    "\n",
    "df2['prev_ps_1'] = df2.groupby('conm')['P/S'].shift(1) \n",
    "df2['prev_ps_2'] = df2.groupby('conm')['P/S'].shift(2) \n",
    "df2 = df2.dropna(subset = ['prev_ps_1', 'prev_ps_2'])\n",
    "\n",
    "#group 별 회귀\n",
    "for group in df2['CFFA_group'].unique():\n",
    "    group_data = df2[df2['CFFA_group'] == group].sort_values('datacqtr')\n",
    "\n",
    "    #target 회사를 임의로 설정\n",
    "    if (len(group_data['conm'].unique()) > 3): #group내 peer 가 존재하는 경우만 한정\n",
    "        for target_conm in group_data['conm'].unique():\n",
    "            target_data = group_data[group_data['conm'] == target_conm]\n",
    "            peer_data = group_data[group_data['conm'] != target_conm]\n",
    "            for conm in peer_data['conm'].unique():\n",
    "                # 각 peer 회사의 P/S 값을 추출\n",
    "                peer_ps = peer_data[peer_data['conm'] == conm][['datacqtr', 'P/S']]\n",
    "\n",
    "                # target_data와 병합을 위해 컬럼 이름 변경\n",
    "                peer_ps = peer_ps.rename(columns={'P/S': f'peer_ps_{conm}'})\n",
    "\n",
    "                # target_data와 병합\n",
    "                target_data = pd.merge(target_data, peer_ps, on='datacqtr', how='left')\n",
    "\n",
    "            test_start = '2022Q1'\n",
    "            train_data = target_data[target_data['datacqtr'] < test_start]\n",
    "            test_data = target_data[target_data['datacqtr'] >= test_start]\n",
    "\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            from sklearn.metrics import mean_squared_error\n",
    "            import pandas as pd\n",
    "\n",
    "            train_data = train_data.fillna(0)\n",
    "            test_data = test_data.fillna(0)\n",
    "\n",
    "            if(len(test_data) > 0 ) and (len(train_data)>0):\n",
    "                X_train = train_data[['sppey', 'oibdpq', 'ROE', 'txtq', 'prev_ps_1', 'prev_ps_2'] + [col for col in train_data.columns if 'peer_ps_' in col]]\n",
    "                y_train = train_data['P/S']\n",
    "\n",
    "                X_test = test_data[['sppey', 'oibdpq', 'ROE', 'txtq', 'prev_ps_1', 'prev_ps_2'] + [col for col in test_data.columns if 'peer_ps_' in col]]\n",
    "                y_test = test_data['P/S']\n",
    "\n",
    "                # 모델 학습\n",
    "                model = RandomForestRegressor(random_state=42)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # 모델 검증\n",
    "                y_pred = model.predict(X_test)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                now = pd.DataFrame({'group': [group], 'name': [target_conm], 'mae': [mae]})\n",
    "                res = pd.concat([res, now])\n",
    "        \n",
    "res = res.sort_values(by='mae', axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3acb3e16-b8d8-421d-b4dc-e4c554bd7c31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>name</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96.0</td>\n",
       "      <td>AMCON DISTRIBUTING CO</td>\n",
       "      <td>0.029853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.0</td>\n",
       "      <td>FERRELLGAS PARTNERS  -LP</td>\n",
       "      <td>0.036482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221.0</td>\n",
       "      <td>CARDINAL HEALTH INC</td>\n",
       "      <td>0.049483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>307.0</td>\n",
       "      <td>SONIC AUTOMOTIVE INC  -CL A</td>\n",
       "      <td>0.075997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>237.0</td>\n",
       "      <td>SIGMATRON INTERNATIONAL INC</td>\n",
       "      <td>0.077177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   group                         name       mae\n",
       "0   96.0        AMCON DISTRIBUTING CO  0.029853\n",
       "0   83.0     FERRELLGAS PARTNERS  -LP  0.036482\n",
       "0  221.0          CARDINAL HEALTH INC  0.049483\n",
       "0  307.0  SONIC AUTOMOTIVE INC  -CL A  0.075997\n",
       "0  237.0  SIGMATRON INTERNATIONAL INC  0.077177"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eee5105e-eb16-486b-bd61-383a10a0f83a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group\n",
      "99.0         0.229112\n",
      "97.0         0.343500\n",
      "234.0        0.428504\n",
      "186.0        0.548341\n",
      "152.0        0.571684\n",
      "             ...     \n",
      "50.0       614.895404\n",
      "244.0     1893.292084\n",
      "36.0      2537.565184\n",
      "245.0     3364.298231\n",
      "248.0    20359.156916\n",
      "Name: mae, Length: 122, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "grouped_result = res.groupby('group')['mae'].mean().sort_values(ascending=True)\n",
    "print(grouped_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "debebbd1-9082-4988-9e37-9ffa60b0ad75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mae가 낮은 3개의 군집:\n",
      "99.0: ['AUTONATION INC' 'PENSKE AUTOMOTIVE GROUP INC' 'WESCO INTL INC'\n",
      " 'LKQ CORP']\n",
      "97.0: ['APPLIED INDUSTRIAL TECH INC' 'LANDSTAR SYSTEM INC' 'TITAN MACHINERY INC'\n",
      " 'QUAD/GRAPHICS INC']\n",
      "234.0: ['INTERFACE INC' 'ASTEC INDUSTRIES INC' 'WABASH NATIONAL CORP'\n",
      " 'ENCORE WIRE CORP' 'COMPASS DIVERSIFIED HOLDINGS']\n",
      "\n",
      " Mae가 높은 3개의 군집:\n",
      "36.0: ['HERON THERAPEUTICS INC' 'RIGEL PHARMACEUTICALS INC'\n",
      " 'LEXICON PHARMACEUTICALS INC' 'CORCEPT THERAPEUTICS INC'\n",
      " 'CELLDEX THERAPEUTICS INC']\n",
      "245.0: ['PROTHENA CORPORATION PLC' 'INTRA-CELLULAR THERAPIES INC' 'INSMED INC'\n",
      " 'ACADIA PHARMACEUTICALS INC' 'MIRATI THERAPEUTICS INC'\n",
      " 'AMICUS THERAPEUTICS INC']\n",
      "248.0: ['GYRE THERAPEUTICS INC' 'ALTIMMUNE INC' 'MOLECULAR TEMPLATES'\n",
      " 'VAXART INC' 'LUMOS PHARMA INC' 'ALAUNOS THERAPEUTICS INC'\n",
      " 'CATALYST PHARMACEUTICALS INC' 'ASSEMBLY BIOSCIENCES INC' 'VERASTEM INC']\n"
     ]
    }
   ],
   "source": [
    "top_5_cff_groups = grouped_result.head(3).index\n",
    "bottom_5_cff_groups = grouped_result.tail(3).index\n",
    "\n",
    "top_5_comn = df2[df2['CFFA_group'].isin(top_5_cff_groups)]['conm'].unique()\n",
    "bottom_5_comn = df2[df2['CFFA_group'].isin(bottom_5_cff_groups)]['conm'].unique()\n",
    "\n",
    "print(\"Mae가 낮은 3개의 군집:\")\n",
    "for cff_group in top_5_cff_groups:\n",
    "    print(f\"{cff_group}: {df2[df2['CFFA_group'] == cff_group]['conm'].unique()}\")\n",
    "\n",
    "print(\"\\n Mae가 높은 3개의 군집:\")\n",
    "for cff_group in bottom_5_cff_groups:\n",
    "    print(f\"{cff_group}: {df2[df2['CFFA_group'] == cff_group]['conm'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25eab25-70b1-41f4-bd21-6bc716461418",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a30601-6228-4345-a2b5-b72211e99f86",
   "metadata": {},
   "source": [
    "### - Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabc31bb-dc09-46fb-b5ef-393ba8cbc581",
   "metadata": {},
   "source": [
    "<oi>\n",
    "    산업 분류 결과 대체로 기존 SIC 보다 세분류 되는 것을 확인할 수 있었다. 허나 기존의 선행연구와 마찬가지로 Across/within std 비교는 하지 못하였기에, 정확도에 대한 의문점은 존재한다. <br>\n",
    "    기존 CFFA 방식의 weight(W) 계산 뿐만 아니라, 시계열 데이터를 기반으로 한 머신러닝 분석도 진행을 해 보았다. 그 결과 weight 방식에 비해선 전체적인 MAE가 대체적으로 더 낮아지는 것으로 나타났다. <br>\n",
    "    즉, business description에서 나온 전략적 자원들이 비슷한 기업들로 분류를 하였을 때, 이를 기반으로 peer group을 형성한다면 어느 정도의 기업 가치 추정이 가능하다는 것을 의미하며 <br>\n",
    "    스타트업 회사, 유니콘 등도 사업 보고서에 기재된 내용을 바탕으로 추정이 가능하게 된다는 것을 의미한다. <br>\n",
    "    <br>\n",
    "    <b> 한계 및 개선점 </b>\n",
    "    1. 현재 선행 연구들에 비해서 데이터 셋이 너무 적다. 이는 컴퓨터 성능, 시간 등의 문제가 있기 때문이다.<br>\n",
    "    2. 현재 랜덤 포레스트 모형에 기반하였으나, 또 다른 머신러닝 기법도 추가적으로 진행하면 좋을 것 이다.<br>\n",
    "    3. 분류된 산업들에 대한 정확도를 알 수 있는 분석 방법이 필요하다.<br>\n",
    "    4. 기업 특성 변수를 더 많이 추가하여 엄밀한 분석이 되야 한다.<br>\n",
    "    5. 다른 텍스트 마이닝 기법 등을 결합하여서, 중요도에 대한 지표를 알 수 있으면 좋을 것 이다.\n",
    "    6. Business 등 모든 기업에서 쓰이는 공통 단어들이 너무 많이 잡혀 unique 단어로써의 기능을 하지 못한다. 그렇기에 전체 기업에서 50~60% 정도가 겹치는 공통 단어 등을 제외하여 unique 단어를 추출하는 방법 역시 필요하다. <br>\n",
    "    7. 현재 선행연구와의 차별점이 너무 약하다. 차별을 둘 수 있는 방안에 대해서 고민이 필요하다. <br>\n",
    "    8. 특정 수 산업 분류가 아닌, 각 회사별로 cosine distance가 가까운 peer 몇 개만을 선정하는 방법 등이 필요해 보인다. 단순 clustering 시, 그 cluster 집단 크기에 대한 의문이 존재하기 때문이다.\n",
    "    9. 단어 빈도 수가 아닌 전체 문구에서 얼마만큼 나왔는지 등의 ratio 등으로 상위 단어 추출이 필요하다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b2805a-5fde-4c91-b207-42b9b3a43223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
