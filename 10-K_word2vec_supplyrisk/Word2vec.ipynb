{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "233731c4-9d5b-472d-8c39-5d023e36dcce",
   "metadata": {},
   "source": [
    "<io>\n",
    "Word2vec 은 원핫인코딩을 기반으로, 단어간 유사도를 파악할 수 있도록 하는 벡터화 방법이다.<br>\n",
    "이를 기반으로, 10-K 에서 사업설명 부분에서 각 기업의 사업설명, 전략적 자원 식별을 바탕으로 비슷한 위치에서 등장하는 단어가 비슷한 의미라는 것을 이용하여 <br>\n",
    "전체 10-K 단어집을 완성시켜보고자 한다. 이를 토대로, 이후 여러 단어집 형성에도 도움이 될 것으로 예상된다. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6660e0-7af3-484e-949f-23b59ef8f1aa",
   "metadata": {},
   "source": [
    "### CBOW (Continous Bag of Words))\n",
    "<oi>\n",
    "주변의 있는 단어 기준으로 윈도우를 형성하여 중심 단어의 벡터를 예측/학습 하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4041e75-979f-4555-978b-83e37f02d35c",
   "metadata": {},
   "source": [
    "### 먼저 한개 문서 (사업설명 부분)에 대해서 연습을 해 보고, 전체 문서로 단어집을 형성해보고자 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0186ef2-9858-4f37-a598-932f231773f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "dir = \"/media/siro8458/Data/본체/자산평가/sentence/\"\n",
    "files = os.listdir(dir)\n",
    "i = 0\n",
    "\n",
    "with open(dir+files[i], 'r', encoding='utf-8') as file:\n",
    "    file_content = file.read()\n",
    "file_name = files[i]\n",
    "# 연도 추출 - 문자 사이의 4자리 숫자임\n",
    "year_match = re.search(r'\\b\\d{4}\\b', file_name)\n",
    "year = year_match.group() if year_match else None\n",
    "# 회사 이름 추출 (연도 바로 앞의 - 이전이 전부 회사이름임)\n",
    "if year:\n",
    "    firmname = file_name.split(year)[0].rstrip('-')\n",
    "else:\n",
    "    firmname = None\n",
    "    \n",
    "file_content = file_content.split('\\n') #\\n 형태로 메모장에 기록되어 있으므로 나누어줌\n",
    "tokenList = [sent.split() for sent in file_content]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "processed_stemmed_list = []\n",
    "for sentence in tokenList:\n",
    "    processed_sentence = []\n",
    "    for word in sentence:\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        processed_sentence.append(stemmed_word)\n",
    "    processed_stemmed_list.append(processed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43d31ffd-5ade-4fdb-9b6c-4558a3d31a8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['miez', 'inc'],\n",
       " ['includ',\n",
       "  'it',\n",
       "  'wholli',\n",
       "  'own',\n",
       "  'subsidiari',\n",
       "  'is',\n",
       "  'a',\n",
       "  'lead',\n",
       "  'specialti',\n",
       "  'retail',\n",
       "  'of',\n",
       "  'apparel',\n",
       "  'footwear',\n",
       "  'accessori',\n",
       "  'and',\n",
       "  'hardgood',\n",
       "  'for',\n",
       "  'young',\n",
       "  'men',\n",
       "  'and',\n",
       "  'women',\n",
       "  'who',\n",
       "  'want',\n",
       "  'to',\n",
       "  'express',\n",
       "  'their',\n",
       "  'individu',\n",
       "  'through',\n",
       "  'the',\n",
       "  'fashion',\n",
       "  'music',\n",
       "  'art',\n",
       "  'and',\n",
       "  'cultur',\n",
       "  'of',\n",
       "  'action',\n",
       "  'sport',\n",
       "  'streetwear',\n",
       "  'and',\n",
       "  'other',\n",
       "  'uniqu',\n",
       "  'lifestyl'],\n",
       " ['zumiez', 'inc']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_stemmed_list[:3] #위에서 부터 3개만 간단히 봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef4b0b95-1607-42e1-900b-f574a76f25a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zumiez Inc-2015.0.txt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b3308-63f4-4bc3-bd73-6745bf00265a",
   "metadata": {},
   "source": [
    "<io>\n",
    "Word2vec 학습 <br>\n",
    "window 옵션이 윈도우 창을 지정해주며, min_count 옵션은 최소 빈도 단어 수로, 이 이하의 빈도수의 경우 학습에 사용안됨 <br>\n",
    "sg 옵션의 경우, 0일 경우 CBOW, 1일 경우 Skip-Gram 으로 현재는 0을 지정하여 학습을 진행하여 CBOW 의 결과를 보고자 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99f2af8a-a3d0-4c98-b399-20c32e0b0452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "cbow_model1 = Word2Vec(sentences=processed_stemmed_list, window=5, min_count=3,  sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4572cc69-eac6-40ce-934f-7faa7f40762f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.999642550945282),\n",
       " ('our', 0.9996387362480164),\n",
       " ('other', 0.9996066093444824),\n",
       " ('and', 0.9996013045310974),\n",
       " ('to', 0.9995980858802795),\n",
       " ('in', 0.999582052230835),\n",
       " ('of', 0.9995809197425842),\n",
       " ('a', 0.9995554685592651),\n",
       " ('store', 0.9995415210723877),\n",
       " ('or', 0.9995406270027161)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model1.wv.most_similar(\"product\") #product와 유사한 단어를 추출해본 결과, stop word에 속하는 단어가 많은 듯 하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caac69e-6cbd-45ee-981c-ee58249ff89a",
   "metadata": {},
   "source": [
    "<io>\n",
    "현재 stopword에 속할마한 the, and 등이 너무 많이 잡히는 탓에 한번 제거를 한 결과로 토대로 다시 해 보고자 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a10abd2-5d5d-44ea-a217-b01476919be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/siro8458/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#처음 켰을때 한번만 실행\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed_stemmed_list2 = []\n",
    "for sentence in tokenList:\n",
    "    processed_sentence = []\n",
    "    for word in sentence:\n",
    "        if not (word in stop_words or re.match(r'\\d+', word)):  #stop_word나 숫자를 제거, 즉 아닌경우\n",
    "            # Porter Stemmer 적용\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            processed_sentence.append(stemmed_word)\n",
    "    processed_stemmed_list2.append(processed_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d1b392b-daa9-4fba-b686-cf2f38c5ddc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['miez', 'inc'],\n",
       " ['includ',\n",
       "  'wholli',\n",
       "  'own',\n",
       "  'subsidiari',\n",
       "  'lead',\n",
       "  'specialti',\n",
       "  'retail',\n",
       "  'apparel',\n",
       "  'footwear',\n",
       "  'accessori',\n",
       "  'hardgood',\n",
       "  'young',\n",
       "  'men',\n",
       "  'women',\n",
       "  'want',\n",
       "  'express',\n",
       "  'individu',\n",
       "  'fashion',\n",
       "  'music',\n",
       "  'art',\n",
       "  'cultur',\n",
       "  'action',\n",
       "  'sport',\n",
       "  'streetwear',\n",
       "  'uniqu',\n",
       "  'lifestyl']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_stemmed_list2[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "418e9bae-ced5-463b-ab7f-38d0f3eac210",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model2 = Word2Vec(sentences=processed_stemmed_list2, window=5, min_count=3,  sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7dc219e-89c8-4eb8-9a42-a83abcea3cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.9996445178985596),\n",
       " ('our', 0.9996435642242432),\n",
       " ('other', 0.9996076822280884),\n",
       " ('and', 0.9996013045310974),\n",
       " ('to', 0.9995996952056885),\n",
       " ('in', 0.9995824098587036),\n",
       " ('of', 0.9995821118354797),\n",
       " ('a', 0.9995564222335815),\n",
       " ('or', 0.9995422959327698),\n",
       " ('store', 0.999542236328125)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model1.wv.most_similar(\"product\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df2eab45-9e47-4e05-889f-368fa75bfefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('control', 0.950051486492157),\n",
       " ('us', 0.9492754340171814),\n",
       " ('could', 0.9425510168075562),\n",
       " ('store', 0.9416999816894531),\n",
       " ('oper', 0.9382789731025696),\n",
       " ('cost', 0.9377127885818481),\n",
       " ('busi', 0.9341433048248291),\n",
       " ('merchandis', 0.9321338534355164),\n",
       " ('may', 0.9300434589385986),\n",
       " ('risk', 0.927400529384613)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model2.wv.most_similar(\"product\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86d6b9ab-fcb9-484b-a794-a6476652b00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('merchandis', 0.9993808269500732)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1 = cbow_model1.wv.most_similar(\"product\", topn=1000)  #topn 옵션을 넣으면 상위 몇개 유사 단어를 볼지를 확인 가능, default는 10\n",
    "next(((word, similarity) for word, similarity in words1 if word == \"merchandis\"), None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14ffa6-02b1-45b3-9a44-6553262d2a4f",
   "metadata": {},
   "source": [
    "<io>\n",
    "    product 와 유사한 단어로 상위 1000개 단어집을 형성하는 경우, stopwords를 하지 않더라도 똑같이 merchandis가 출력이 되는 것으로 보인다.<br>\n",
    "    stopword 여부는 의미가 없을 수 있지만, 현재 한개 문장에는 단어 수 등의 표본이 적을 수 있기에, 더 많은 표본을 넣고, stopword 와 단어집을 한번 비교를 해보도록 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce34f89a-f727-4bef-a72b-1f4023f56e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/siro8458/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#처음 켰을때 한번만 실행\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "dir = \"/media/siro8458/Data/본체/자산평가/sentence/\"\n",
    "files = os.listdir(dir)\n",
    "\n",
    "processed_stemmed_list1 = []\n",
    "processed_stemmed_list2 = []\n",
    "\n",
    "\n",
    "for i in range(0, len(files)):\n",
    "    with open(dir+files[i], 'r', encoding='utf-8') as file:\n",
    "        file_content = file.read()\n",
    "    file_name = files[i]\n",
    "    # 연도 추출 - 문자 사이의 4자리 숫자임\n",
    "    year_match = re.search(r'\\b\\d{4}\\b', file_name)\n",
    "    year = year_match.group() if year_match else None\n",
    "    # 회사 이름 추출 (연도 바로 앞의 - 이전이 전부 회사이름임)\n",
    "    if year:\n",
    "        firmname = file_name.split(year)[0].rstrip('-')\n",
    "    else:\n",
    "        firmname = None\n",
    "        \n",
    "    file_content = file_content.split('\\n') #\\n 형태로 메모장에 기록되어 있으므로 나누어줌\n",
    "    tokenList = [sent.split() for sent in file_content]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    \n",
    "    for sentence in tokenList:\n",
    "        processed_sentence1 = []\n",
    "        processed_sentence2 = []\n",
    "        for word in sentence:\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            processed_sentence1.append(stemmed_word)\n",
    "        processed_stemmed_list1.append(processed_sentence1)\n",
    "\n",
    "        for word in sentence:\n",
    "            if not (word in stop_words or re.match(r'\\d+', word)):  #stop_word나 숫자를 제거, 즉 아닌경우\n",
    "                # Porter Stemmer 적용\n",
    "                stemmed_word = stemmer.stem(word)\n",
    "                processed_sentence2.append(stemmed_word)\n",
    "        processed_stemmed_list2.append(processed_sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2827df4-4359-4214-87eb-c681c5c002cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['miez', 'inc'],\n",
       " ['includ',\n",
       "  'wholli',\n",
       "  'own',\n",
       "  'subsidiari',\n",
       "  'lead',\n",
       "  'specialti',\n",
       "  'retail',\n",
       "  'apparel',\n",
       "  'footwear',\n",
       "  'accessori',\n",
       "  'hardgood',\n",
       "  'young',\n",
       "  'men',\n",
       "  'women',\n",
       "  'want',\n",
       "  'express',\n",
       "  'individu',\n",
       "  'fashion',\n",
       "  'music',\n",
       "  'art',\n",
       "  'cultur',\n",
       "  'action',\n",
       "  'sport',\n",
       "  'streetwear',\n",
       "  'uniqu',\n",
       "  'lifestyl'],\n",
       " ['zumiez', 'inc'],\n",
       " ['form', 'august', 'washington', 'state', 'corpor'],\n",
       " ['januari', 'oper', 'store', 'unit', 'state', 'us', 'canada', 'europ']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_stemmed_list2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b03d7e-e31e-44c5-8bf7-16c9036d4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model1 = Word2Vec(sentences=processed_stemmed_list1, window=5, min_count=3,  sg=0)\n",
    "cbow_model2 = Word2Vec(sentences=processed_stemmed_list2, window=5, min_count=3,  sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b211b76e-6277-4fb6-97cc-da58d02dca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar1 = cbow_model1.wv.most_similar(\"product\", topn=1000)\n",
    "similar2 = cbow_model2.wv.most_similar(\"product\", topn=1000)\n",
    "\n",
    "f_words1= [word for word, similarity in similar1 if similarity > 0.3]\n",
    "f_words2= [word for word, similarity in similar2 if similarity > 0.3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ca81f66-791a-46df-9352-f88b274987e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>add_stop</th>\n",
       "      <th>not_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drug</td>\n",
       "      <td>manufactur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>merchandis</td>\n",
       "      <td>drug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vaccin</td>\n",
       "      <td>merchandis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>manufactur</td>\n",
       "      <td>vaccin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>solut</td>\n",
       "      <td>microorgan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>NaN</td>\n",
       "      <td>clickpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>NaN</td>\n",
       "      <td>suppositori</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>NaN</td>\n",
       "      <td>radiometr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>NaN</td>\n",
       "      <td>polycarbon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>NaN</td>\n",
       "      <td>depomeloxicam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>793 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       add_stop       not_stop\n",
       "0          drug     manufactur\n",
       "1    merchandis           drug\n",
       "2        vaccin     merchandis\n",
       "3    manufactur         vaccin\n",
       "4         solut     microorgan\n",
       "..          ...            ...\n",
       "788         NaN       clickpad\n",
       "789         NaN    suppositori\n",
       "790         NaN      radiometr\n",
       "791         NaN     polycarbon\n",
       "792         NaN  depomeloxicam\n",
       "\n",
       "[793 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d1 = pd.DataFrame(f_words1, columns = (['add_stop']))\n",
    "d2 = pd.DataFrame(f_words2, columns = (['not_stop']))\n",
    "pd.concat([d1, d2], axis=1) #stop_words가 없는 경우 더 단어가 많이 뽑히는 것처럼 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6595e628-b52f-4b50-81b2-0a30f51b6cc5",
   "metadata": {},
   "source": [
    "<oi>\n",
    " 각 회사의 대표적인 상품들이 들어와 있는 것으로 추정되며, CBOW 방식을 이용하였을 경우 어느정도의 각 회사의 상품으로 추정되는 문구를 찾는 듯 하다.<br>\n",
    " 또한 여러 회사들에서 하는 것과 마찬가지로 similarity가 0.3이상인 결과도 다음과 같고, skip-gram까지 합쳐서 risk와 supply 관련 단어를 통하여 각 회사별 supply_risk 추정할 계획"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48c18a8-a0dc-4d79-a436-cdad2b4bc9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW_add_stop :  360.00419068336487\n",
      "CBOW_not_stop :  265.51319313049316\n",
      "SKIP_add_stop :  1200.4223911762238\n",
      "SKIP_not_stop :  833.9526751041412\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "start_time = time.time()\n",
    "cbow_model1 = Word2Vec(sentences=processed_stemmed_list1, window=5, min_count=3,  sg=0)\n",
    "end_time = time.time()\n",
    "total_time = (end_time - start_time)\n",
    "print(\"CBOW_add_stop : \", total_time)\n",
    "\n",
    "start_time = time.time()\n",
    "cbow_model2 = Word2Vec(sentences=processed_stemmed_list2, window=5, min_count=3,  sg=0) \n",
    "end_time = time.time()\n",
    "total_time = (end_time - start_time)\n",
    "print(\"CBOW_not_stop : \", total_time)\n",
    "\n",
    "start_time = time.time()\n",
    "cbow_model3 = Word2Vec(sentences=processed_stemmed_list1, window=5, min_count=3,  sg=1) #skip_words\n",
    "end_time = time.time()\n",
    "total_time = (end_time - start_time)\n",
    "print(\"SKIP_add_stop : \", total_time)\n",
    "\n",
    "start_time = time.time()\n",
    "cbow_model4 = Word2Vec(sentences=processed_stemmed_list2, window=5, min_count=3,  sg=1) #skip_words_except stop words\n",
    "end_time = time.time()\n",
    "total_time = (end_time - start_time)\n",
    "print(\"SKIP_not_stop : \", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05586b5e-db74-4bcb-b5ca-7fa14e96f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supply :  suppli\n",
      "risk :  risk\n"
     ]
    }
   ],
   "source": [
    "print(\"supply : \", stemmer.stem(\"supply\"))\n",
    "print(\"risk : \", stemmer.stem(\"risk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3ef7243-23e6-48de-8264-42be648a5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "supply1 = cbow_model1.wv.most_similar(\"suppli\", topn=1000)\n",
    "supply2 = cbow_model2.wv.most_similar(\"suppli\", topn=1000)\n",
    "supply3 = cbow_model3.wv.most_similar(\"suppli\", topn=1000)\n",
    "supply4 = cbow_model4.wv.most_similar(\"suppli\", topn=1000)\n",
    "\n",
    "s_words1= [word for word, similarity in supply1 if similarity > 0.3]\n",
    "s_words2= [word for word, similarity in supply2 if similarity > 0.3]\n",
    "s_words3= [word for word, similarity in supply3 if similarity > 0.3]\n",
    "s_words4= [word for word, similarity in supply4 if similarity > 0.3]\n",
    "\n",
    "s1 = pd.DataFrame(f_words1, columns = (['add_stop_CBOW']))\n",
    "s2 = pd.DataFrame(f_words2, columns = (['not_stop_CBOW']))\n",
    "s3 = pd.DataFrame(f_words1, columns = (['add_stop_SKIP']))\n",
    "s4 = pd.DataFrame(f_words2, columns = (['not_stop_SKIP']))\n",
    "supply = pd.concat([s1,s2,s3,s4], axis=1) \n",
    "\n",
    "risk1 = cbow_model1.wv.most_similar(\"risk\", topn=1000)\n",
    "risk2 = cbow_model2.wv.most_similar(\"risk\", topn=1000)\n",
    "risk3 = cbow_model3.wv.most_similar(\"risk\", topn=1000)\n",
    "risk4 = cbow_model4.wv.most_similar(\"risk\", topn=1000)\n",
    "\n",
    "r_words1= [word for word, similarity in risk1 if similarity > 0.3]\n",
    "r_words2= [word for word, similarity in risk2 if similarity > 0.3]\n",
    "r_words3= [word for word, similarity in risk3 if similarity > 0.3]\n",
    "r_words4= [word for word, similarity in risk4 if similarity > 0.3]\n",
    "\n",
    "r1 = pd.DataFrame(r_words1, columns = (['add_stop_CBOW']))\n",
    "r2 = pd.DataFrame(r_words2, columns = (['not_stop_CBOW']))\n",
    "r3 = pd.DataFrame(r_words1, columns = (['add_stop_SKIP']))\n",
    "r4 = pd.DataFrame(r_words2, columns = (['not_stop_SKIP']))\n",
    "risk = pd.concat([r1,r2,r3,r4], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3216cac0-1e12-4f69-825d-0f700c191bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>add_stop_CBOW</th>\n",
       "      <th>not_stop_CBOW</th>\n",
       "      <th>add_stop_SKIP</th>\n",
       "      <th>not_stop_SKIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drug</td>\n",
       "      <td>manufactur</td>\n",
       "      <td>drug</td>\n",
       "      <td>manufactur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>merchandis</td>\n",
       "      <td>drug</td>\n",
       "      <td>merchandis</td>\n",
       "      <td>drug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vaccin</td>\n",
       "      <td>merchandis</td>\n",
       "      <td>vaccin</td>\n",
       "      <td>merchandis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>manufactur</td>\n",
       "      <td>vaccin</td>\n",
       "      <td>manufactur</td>\n",
       "      <td>vaccin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>solut</td>\n",
       "      <td>microorgan</td>\n",
       "      <td>solut</td>\n",
       "      <td>microorgan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>NaN</td>\n",
       "      <td>clickpad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clickpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>NaN</td>\n",
       "      <td>suppositori</td>\n",
       "      <td>NaN</td>\n",
       "      <td>suppositori</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>NaN</td>\n",
       "      <td>radiometr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>radiometr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>NaN</td>\n",
       "      <td>polycarbon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polycarbon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>NaN</td>\n",
       "      <td>depomeloxicam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>depomeloxicam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>793 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    add_stop_CBOW  not_stop_CBOW add_stop_SKIP  not_stop_SKIP\n",
       "0            drug     manufactur          drug     manufactur\n",
       "1      merchandis           drug    merchandis           drug\n",
       "2          vaccin     merchandis        vaccin     merchandis\n",
       "3      manufactur         vaccin    manufactur         vaccin\n",
       "4           solut     microorgan         solut     microorgan\n",
       "..            ...            ...           ...            ...\n",
       "788           NaN       clickpad           NaN       clickpad\n",
       "789           NaN    suppositori           NaN    suppositori\n",
       "790           NaN      radiometr           NaN      radiometr\n",
       "791           NaN     polycarbon           NaN     polycarbon\n",
       "792           NaN  depomeloxicam           NaN  depomeloxicam\n",
       "\n",
       "[793 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db2c9a12-e129-4ae2-b790-f155ec217ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>add_stop_CBOW</th>\n",
       "      <th>not_stop_CBOW</th>\n",
       "      <th>add_stop_SKIP</th>\n",
       "      <th>not_stop_SKIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exposur</td>\n",
       "      <td>exposur</td>\n",
       "      <td>exposur</td>\n",
       "      <td>exposur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>riskth</td>\n",
       "      <td>riskth</td>\n",
       "      <td>riskth</td>\n",
       "      <td>riskth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>riskw</td>\n",
       "      <td>riskw</td>\n",
       "      <td>riskw</td>\n",
       "      <td>riskw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11risk</td>\n",
       "      <td>uncertainti</td>\n",
       "      <td>11risk</td>\n",
       "      <td>uncertainti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22risk</td>\n",
       "      <td>risksw</td>\n",
       "      <td>22risk</td>\n",
       "      <td>risksw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>riskier</td>\n",
       "      <td>software2014bannomobil</td>\n",
       "      <td>riskier</td>\n",
       "      <td>software2014bannomobil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>hurdl</td>\n",
       "      <td>gaap_residentialmortgagememb</td>\n",
       "      <td>hurdl</td>\n",
       "      <td>gaap_residentialmortgagememb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>gleam</td>\n",
       "      <td>riskier</td>\n",
       "      <td>gleam</td>\n",
       "      <td>riskier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>company4t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>company4t</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>rmb63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rmb63</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   add_stop_CBOW                 not_stop_CBOW add_stop_SKIP  \\\n",
       "0        exposur                       exposur       exposur   \n",
       "1         riskth                        riskth        riskth   \n",
       "2          riskw                         riskw         riskw   \n",
       "3         11risk                   uncertainti        11risk   \n",
       "4         22risk                        risksw        22risk   \n",
       "..           ...                           ...           ...   \n",
       "74       riskier        software2014bannomobil       riskier   \n",
       "75         hurdl  gaap_residentialmortgagememb         hurdl   \n",
       "76         gleam                       riskier         gleam   \n",
       "77     company4t                           NaN     company4t   \n",
       "78         rmb63                           NaN         rmb63   \n",
       "\n",
       "                   not_stop_SKIP  \n",
       "0                        exposur  \n",
       "1                         riskth  \n",
       "2                          riskw  \n",
       "3                    uncertainti  \n",
       "4                         risksw  \n",
       "..                           ...  \n",
       "74        software2014bannomobil  \n",
       "75  gaap_residentialmortgagememb  \n",
       "76                       riskier  \n",
       "77                           NaN  \n",
       "78                           NaN  \n",
       "\n",
       "[79 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5fa4211-80ae-4c67-94f7-d2274c0cdee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "supply.to_csv('supply.csv',index=False)\n",
    "risk.to_csv('risk.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fcd853-3c9a-461a-b14f-086c8e6653e5",
   "metadata": {},
   "source": [
    "<io>\n",
    "skip 과 cbow 모두 비슷하게 뜨는 듯 하다. 또한, supply의 경우 이전 product와 유사한 단어가 뜨는 듯 하며, 단순 수출이 아닌 상품에 대한 ㄹ과를 보이는 듯 하다<br>\n",
    "    구성된 단어집을 바탕으로 각 기업의 전체 token 에서 supply risk의 비율을 계산해 보고자 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "724d7a5d-f17d-429c-9b5e-1c27ceecf291",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/media/siro8458/Data/본체/자산평가/sentence/\"\n",
    "files = os.listdir(dir)\n",
    "\n",
    "processed_stemmed_list1 = []\n",
    "supply_risk = {}\n",
    "\n",
    "add_stop_CBOW_s = supply['add_stop_CBOW'].dropna().tolist()\n",
    "not_stop_CBOW_s = supply['not_stop_CBOW'].dropna().tolist()\n",
    "add_stop_SKIP_s = supply['add_stop_SKIP'].dropna().tolist()\n",
    "not_stop_SKIP_s = supply['not_stop_SKIP'].dropna().tolist()\n",
    "\n",
    "add_stop_CBOW_r = risk['add_stop_CBOW'].dropna().tolist()\n",
    "not_stop_CBOW_r = risk['not_stop_CBOW'].dropna().tolist()\n",
    "add_stop_SKIP_r = risk['add_stop_SKIP'].dropna().tolist()\n",
    "not_stop_SKIP_r = risk['not_stop_SKIP'].dropna().tolist()\n",
    "\n",
    "\n",
    "for i in range(0,len(files)):\n",
    "    with open(dir+files[i], 'r', encoding='utf-8') as file:\n",
    "        file_content = file.read()\n",
    "    file_name = files[i]\n",
    "    # 연도 추출 - 문자 사이의 4자리 숫자임\n",
    "    year_match = re.search(r'\\b\\d{4}\\b', file_name)\n",
    "    year = year_match.group() if year_match else None\n",
    "    # 회사 이름 추출 (연도 바로 앞의 - 이전이 전부 회사이름임)\n",
    "    if year:\n",
    "        firmname = file_name.split(year)[0].rstrip('-')\n",
    "    else:\n",
    "        firmname = None\n",
    "\n",
    "    key = f\"{firmname}-{year}\"\n",
    "    file_content = file_content.split('\\n') #\\n 형태로 메모장에 기록되어 있으므로 나누어줌\n",
    "    tokenList = [sent.split() for sent in file_content]\n",
    "       \n",
    "    \n",
    "    sr_count1 = 0\n",
    "    sr_count2 = 0\n",
    "    sr_count3 = 0\n",
    "    sr_count4 = 0 #각 문서별 전체 suupply risk 수 카운트 위해서    \n",
    "    for sentence in tokenList:\n",
    "        s_count1 = 0 #각 문장별 supply랑 risk 여부 확인하기 위해서\n",
    "        r_count1 = 0\n",
    "        s_count2 = 0\n",
    "        r_count2 = 0\n",
    "        s_count3 = 0\n",
    "        r_count3 = 0\n",
    "        s_count4 = 0\n",
    "        r_count4 = 0\n",
    "        for word in sentence:\n",
    "            #STOP WORD 포함된 CBOW\n",
    "            if word in add_stop_CBOW_s:\n",
    "                s_count1+=1 #그 문장에 suupply 단어가 있으면 +1\n",
    "            if word in add_stop_CBOW_r:\n",
    "                r_count1+=1 #그 문장에 risk 단어가 있으면 +1\n",
    "            #STOP WORD 제거된 CBOW\n",
    "            if word in not_stop_CBOW_s:\n",
    "                s_count2+=1 #그 문장에 suupply 단어가 있으면 +1\n",
    "            if word in not_stop_CBOW_r:\n",
    "                r_count2+=1 #그 문장에 risk 단어가 있으면 +1\n",
    "            #STOP WORD 포함된 SKIP\n",
    "            if word in add_stop_SKIP_s:\n",
    "                s_count3+=1 #그 문장에 suupply 단어가 있으면 +1\n",
    "            if word in add_stop_SKIP_r:\n",
    "                r_count3+=1 #그 문장에 risk 단어가 있으면 +1\n",
    "            #STOP WORD 제거된 SKIP\n",
    "            if word in not_stop_SKIP_s:\n",
    "                s_count4+=1 #그 문장에 suupply 단어가 있으면 +1\n",
    "            if word in not_stop_SKIP_r:\n",
    "                r_count4+=1 #그 문장에 risk 단어가 있으면 +1\n",
    "        if s_count1*r_count1 > 0: #문장안에서 반복문이 끝났으니, 둘 곱이 0이 아니면 supply와 risk 단어가 모두 나타난다는 뜻\n",
    "            sr_count1+=1 #supply risk +1 해줌\n",
    "        if s_count2*r_count2 > 0: \n",
    "            sr_count2+=1 \n",
    "        if s_count3*r_count3 > 0: \n",
    "            sr_count3+=1 \n",
    "        if s_count4*r_count4 > 0: \n",
    "            sr_count4+=1 \n",
    "    supply_risk[key] = {'sr_add_stop_CBOW' : sr_count1/len(tokenList), 'sr_not_stop_CBOW' : sr_count2/len(tokenList),\n",
    "                       'sr_add_stop_SKIP' : sr_count3/len(tokenList), 'sr_not_stop_SKIP' : sr_count4/len(tokenList),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb277f74-1357-40aa-ba74-88603c83e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('supply_risk.pkl', 'wb') as f:\n",
    "    pickle.dump(supply_risk, f)\n",
    "    \n",
    "with open('supply_risk.pkl', 'rb') as f:\n",
    "    supply_risk = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db4daf-72a4-4105-941c-1b734fef13a1",
   "metadata": {},
   "source": [
    "### 기존의 CAR 등과 같이 모아놓은거랑 합쳐서 분석을 진행해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "731a53c5-df6e-4a23-bd0f-b27bfe808cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd = pd.read_csv('CAR_control2.csv', encoding='cp949', index_col=0)\n",
    "ss = pd.read_csv('gram_sample_CIK_filed.csv', encoding='cp949')\n",
    "\n",
    "sr_add_stop_CBOW = []\n",
    "sr_not_stop_CBOW = []\n",
    "sr_add_stop_SKIP = []\n",
    "sr_not_stop_SKIP = []\n",
    "\n",
    "for i in range(0, len(ddd)):\n",
    "    key = ss.loc[i, 'name']+'-'+str(ss.loc[i, 'year'])\n",
    "    sr_add_stop_CBOW.append(supply_risk[key]['sr_add_stop_CBOW'])\n",
    "    sr_not_stop_CBOW.append(supply_risk[key]['sr_not_stop_CBOW'])\n",
    "    sr_add_stop_SKIP.append(supply_risk[key]['sr_add_stop_SKIP'])\n",
    "    sr_not_stop_SKIP.append(supply_risk[key]['sr_not_stop_SKIP'])\n",
    "\n",
    "sr_add_stop_CBOW = pd.DataFrame(sr_add_stop_CBOW, columns=['sr_add_stop_CBOW'])\n",
    "sr_not_stop_CBOW = pd.DataFrame(sr_not_stop_CBOW, columns=['sr_not_stop_CBOW'])\n",
    "sr_add_stop_SKIP = pd.DataFrame(sr_add_stop_SKIP, columns=['sr_add_stop_SKIP'])\n",
    "sr_not_stop_SKIP = pd.DataFrame(sr_not_stop_SKIP, columns=['sr_not_stop_SKIP'])\n",
    "\n",
    "ddd = pd.concat([ddd, sr_add_stop_CBOW, sr_not_stop_CBOW, sr_add_stop_SKIP, sr_not_stop_SKIP], axis=1)\n",
    "ddd.to_csv('CAR_control_supply_risk.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1ee70089-2e12-401e-bf63-58b5b267dd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================\n",
      "                 sr_add_stop_CBOW sr_not_stop_CBOW sr_add_stop_SKIP sr_not_stop_SKIP\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept        0.0076***        0.0077***        0.0076***        0.0077***       \n",
      "                 (0.0028)         (0.0028)         (0.0028)         (0.0028)        \n",
      "sale             0.0000           0.0000           0.0000           0.0000          \n",
      "                 (0.0000)         (0.0000)         (0.0000)         (0.0000)        \n",
      "lnmkv            -0.0012***       -0.0012***       -0.0012***       -0.0012***      \n",
      "                 (0.0004)         (0.0004)         (0.0004)         (0.0004)        \n",
      "bperhare         0.0000***        0.0000***        0.0000***        0.0000***       \n",
      "                 (0.0000)         (0.0000)         (0.0000)         (0.0000)        \n",
      "sr_add_stop_CBOW -0.0542                                                            \n",
      "                 (0.1495)                                                           \n",
      "sr_not_stop_CBOW                  -0.0590                                           \n",
      "                                  (0.1272)                                          \n",
      "sr_add_stop_SKIP                                   -0.0542                          \n",
      "                                                   (0.1495)                         \n",
      "sr_not_stop_SKIP                                                    -0.0590         \n",
      "                                                                    (0.1272)        \n",
      "R-squared        0.0127           0.0127           0.0127           0.0127          \n",
      "R-squared Adj.   0.0124           0.0124           0.0124           0.0124          \n",
      "====================================================================================\n",
      "Standard errors in parentheses.\n",
      "* p<.1, ** p<.05, ***p<.01\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "import pandas as pd\n",
    "\n",
    "ddd = pd.read_csv('CAR_control_supply_risk.csv', encoding='cp949', index_col=0)\n",
    "\n",
    "X = ['sr_add_stop_CBOW','sr_not_stop_CBOW', 'sr_add_stop_SKIP', 'sr_not_stop_SKIP']\n",
    "\n",
    "models = []\n",
    "\n",
    "for x in X:\n",
    "    if '.' in x: #0.001과 같은 경우 변수명이 안읽혀서 변수명을 새로 하기보단 방법을 찾아보니 {} 형태로 감싸기로 ㅎㅁ\n",
    "        formula = \"CAR ~ sale + lnmkv + bperhare + Q('{}')\".format(x)\n",
    "    else:\n",
    "        formula = 'CAR ~ sale + lnmkv + bperhare + {}'.format(x)\n",
    "    model = smf.ols(formula, data=ddd).fit()\n",
    "    models.append(model)\n",
    "          \n",
    "dfoutput = summary_col(models, stars=True, model_names=X)\n",
    "print(dfoutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dcc0e3-ef26-4309-88b8-dd6858ba4f6a",
   "metadata": {},
   "source": [
    "<io>\n",
    " Supply Risk에 대해서 이전에 구한 공시일 직후의 CAR 1 의 변화에는 유의성이 나타나지 않았으나, 부호는 음의 방향을 나타내었다. <br>\n",
    " 이는 공급망 자체가 장기적인 효과를 추정하기도 하고, 선행 연구들과 같이 volatility가 Cash ratio나 Inventory 를 기반으로 추정을 한다면 유사한 결과가 나타날 듯 하다 <br>\n",
    " 음의 방향이라는 것 자체는 결국 supply risk 역시 위험리스크 를 나타내기에 높으면 높을수록, CAR 등에 음의 영향을 끼친다는 직관과는 일치한다. <br>\n",
    " CBOW와 SKIP의 결과 자체는 결국 단어집 구성에는 차이가 없고, 성능 차이 즉 속도의 차이만을 보였다는 것을 확인 가능하다. (약 4배) <br>\n",
    " 리스크에 대한 언급이 많을 이후 재무지표 설명라인 혹은 어닝콜과 같이 언어적 요소로 구성되는 부분에 대해서 분석을 진행한다면 더욱 유의한 결과를 보일 것으로 기대된다.<br>\n",
    " 추가적인 데이터 셋의 수집 등이 필요할 것으로 분석된다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06770038-e298-4757-a0b2-3075d30b1eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
