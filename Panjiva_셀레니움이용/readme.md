S&P500 의 Panjiva와 같은 경우 수출입 데이터에 대해서 보고서 등과 일별 데이터 등을 볼 수 있다는 장접이 있다. <br>
Xpressfeed 와 같은 것을 통해 데이터를 쉽게 분석에 사용 가능하지만 추가요금이 무지막지 하기에, 기본 보고서와 일별 excel, 혹은 csv 파일을 받는 요금제 수준에서 <br>
데이터 분석 등에 사용하기 위한 10~20년치 미국 수출 기업의 수출량 데이터를 얻는 것은 쉬운 일이 아니다. <br>

그렇기에 그 많은 데이터를 얻기 위하여 셀레니움을 이용하여 멀티프로세스를 통하여 api를 통해 데이터를 긁어오는 방식처럼 <br>
멀티 프로세스를 활용한 웹 크롤링 방식에 대한 코드를 간단히 표시해 두었다. <br>
css_selector나 x_path 등은 지금 코드와 같이 panjiva 사이트가 아니더라고 각 사이트 별로 주소를 통하여 응용을 한다면 비슷하게 적용 가능할 것 이며 <br>
가장 중요한 것은 css_selector가 변하지 않는 고정의 주소값들이 어떤 것인지 그 패턴을 알아내는 것이 중요하다. <br>
웹 크롤링에 있어서, 서버 부하를 막기 위하여 css 주소 자체를 java 등을 통하여 다른 변수와도 얽히게 한 다음에 그 때마다 값이 달라지는 변수 형태인 경우가 존재하기 때문이다.<br>
그렇기에 본 코드에서는 그 날의 값을 받은 다음에는 새로고침을 하는 것이 달력 선택에 있어서 주소값이 변하지 않는 다는 것을 알아내서 활용하였다. <br>

또한 멀티프로세스의 기준은 각 연도 파일을 csv 형태로 미리 만들어서, 각 연도별로 쓰레드 수에 맞게 멀티 프로세스를 진행하도록 하였다. <br>
판지바 자체에서 제약을 거는 경우가 너무 많았으며, 수 많은 버그를 내가 제보한 탓에 웹 크롤링 방식 등은 매크로로 간주되어 계정들이 많이 lock 되었으며, 시행착오 끝에 제한된 시간 내에 방법을 찾아내었다. <br>
<img src = "https://github.com/siro8458/code_practice/assets/156631928/62abc36e-623c-4c15-b947-3901cbe82965">
특정 날의 shipment 혹은 shipper 항목 등을 선정하고, 목적지 등을 설정하는 등의 기본 작업을 거치면 위 사진과 같이 export_results를 누르게 되면 창이 생성이된다. <br>
default 값은 Shipper 를 선택하면 Shipper가 뜨게 되는 방식인데, 여기서 다른 단일 항목을 고르게 되면 반드시 기존 default인 Shipper가 나오게 된다. <br>
즉 다른 항목에 대해서 단일 보고서는 획득을 할 수 없으며, 문의 결과, 다른 보고서를 같이 첨부하라는 연락을 받았을 뿐 이다. <br>

또한, results라 되어 있는 항목은 csv 파일 등으로 다운받게되면 나오는 열의 개수인데, 500까지는 csv 형태로 직접 다운, 그 이상은 최대 10,000개 까지 등록한 e-mail 을 통하여 받을 수 있으나,
기한을 길게 하여 2~3일의 기한의 경우, 기본 데이터 열 수가 3만을 넘어가게 되는 경우가 존재하는데, 3만번째 row 이후에 대해서는 데이터를 제공하지 않으며, <br>
문의 결과, 기술적인 문제고 고쳐보겠다고 하였지만, 돌아온 것은 계정을 lock 걸고 끝났을 뿐 이었다. 3만번째 row 이후에 대해서는 일반 고객은 다운 할 엄두를 못내는데, 코드를 이용하여 확인도 하였고 서버에 부하를 주었다는 것이 이유였다. <br>
그렇기에 코드 내에서 1일 씩으로 끊어서 2만번재 row에 도달 안 하도록 데이터 셋을 구성할 수 있도록 하였으며, 그로 인해서 그 다음날로 넘어가기 위해서 새로고침을 하는 방식을 채택하였다. <br>
(원래는 전체 기한, 혹은 월별로 긁는 방식으로 3만, 4만 등 숫자를 반복문 등을 통해서 늘려가려고 했음) <br>

<img src = "https://github.com/siro8458/code_practice/assets/156631928/d2e73a53-5b2a-4d7d-be6a-be986b31dc7c">
마지막으로 위 사진과 같이 다운로드 항목 내에서 현재 다운로드 진행 상황을 볼 수 있는데, <br>
판지바 자체에서 서버에 요청하는 방식은, 최대 10개까지 다운로드 대기열에 걸 수 있으며, 동시 작업되는 것은 최대 3~4개 정도이다. <br>
또한, row의 수가 많으면 많을 수록 다운로드 수는 비례하여 늘어나게 되기에, 정확히 어느 정도에 작업이 끝나는 지를 알 수 없다. <br>
처음 시도한 방식은, 20년 가까이 되는 데이터와, 각 날짜별로도 데이터가 많기에, 10,000개씩 받는 이메일 방식을 채택하였으나 <br>
그 이전에 많은 버그에 대해서 제보한 탓에 계정을 항상 주의깊게 살펴보는 관심계정이 되어, <br>
10개 이후 대기열에 넣을 수 있을 때 까지, 요청을 계속 하거나, delay를 걸어서 하는 방식을 할 지라도, 정확한 시간 추정이 안되다 보니 서버에 부하를 계속 주기도 하여 <br>
많은 부계정들이 block을 당하였고, 대기열에 3개만 넣어도 그 이후로는 서버 문제라며 lock 을 걸어버린 탓에 이메일 방법으로는 불가능하게 되었으며 <br>
계약기간이 거의 만료가 되어 시간이 없는 상황이었다. <br>

그러던 와중, 사람이 하나하나 다운로드 버튼 누르는 것과, 컴퓨터를 어떻게 구분한거지 라는 생각에 빠진 나는, 이메일 방식으로 대기열에 규칙적으로 거는 것이 아니라 <br>
500개씩 다운로드 버튼을 누르게 하는 방식은 사람이 열심히 하는 것과 같은 방식이 아닌가? 하는 생각이 들어 이를 응용하고자 하였으며, <br>
엑셀로 직접 다운로드 버튼을 누르는 것은 다운로드 대기열에서 작업이 끝날 때 까지 다른 버튼을 누를 수 없다는 것을 알게 된 나는 이를 사용하고자 하였으며, <br>
대기열 자체에서 서로 충돌도 없다는 것을 발견하였다. 그렇기에 급하게 선회하여, 멀티프로세스 방식을 이용하여 각 년도별로 쓰레드 수에 맞게 너무 대기열 충돌은 안나도록 한 5개 정도의 쓰레드를 이용하였으며 <br>
그 결과 현재 계정은 lock도 안되고, 기하급수적으로 작업을 단축 가능하였다. 마치 크롬창을 5개를 켜서 5명이서 동시 작업하는 방식을 이용하였고 수 많은 무거운 데이터를 온전하게 받을 수 있었다. <br>
또한 각 크롬에서 다운되는 방식 역시 각 작업소 별로 나누어서 각 폴더에 저장을 하도록 한 탓에 파일들이 겹치지 않았으며, 이후 가장 큰 raw 파일로 병합하는데 역시 무리가 없었다. <br>
첨부된 코드가 그 작업의 마스터피스인 코드이며, 보안을 위해 개인정보, id 등은 지운 상태이며 이후 웹 크롤링등을 진행해야 한다면 이를 응용하면 시간이 단축 될 것으로 기대된다. <br>
